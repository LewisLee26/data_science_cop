{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning - regression\n",
    "\n",
    "In supervised regression, we want to train the algorithm to predict a continuous value, rather than a categorical value as we do in classification. These techniques are essentially the same as regression in statistics.\n",
    "\n",
    "In this notebook we will look at how to create a regression pipeline using the tools available in the Scikit-learn library. The examples are quite simple, but demonstrate the key steps in doing regression. The interface for each type of class in the process is quite consistent for all the different examples of that type, so even when dealing with much more complicated examples, the interface will be the same and so the code will look similar. See the scikit-learn documentation for more information on the functions used:\n",
    "\n",
    "Scikit-learn documentation\n",
    "* User Guide - Supervised learning: https://scikit-learn.org/stable/supervised_learning.html\n",
    "* API reference: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import mpl_toolkits.mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neural_network\n",
    "import sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and exploration\n",
    "We will use a generated dataset from scikit-learn to demonstrate the regression pipeline. To better demonstrate a standard pipeline, I am changing the mean and standard deviation of the input features to represent a dataset closer to a real example where preprpocessing is required.\n",
    "\n",
    "Documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=10000\n",
    "n_features=10\n",
    "n_informative=5\n",
    "n_targets = 1\n",
    "noise_std = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_random = numpy.repeat(numpy.random.random([1,n_features])*5, n_samples, axis=0)\n",
    "translate_random = numpy.repeat(numpy.array(numpy.random.random([1,n_features])*10-5,dtype=numpy.int32), n_samples, axis=0)\n",
    "input_features, target_feature, coef1 =  sklearn.datasets.make_regression(n_samples=n_samples, \n",
    "                                         n_features=n_features, \n",
    "                                         n_informative=n_informative, \n",
    "                                         n_targets=n_targets, \n",
    "                                         noise=noise_std,\n",
    "                                         coef=True)\n",
    "input_features = (input_features * scale_random) + translate_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'target variable range=[{target_feature.min()},{target_feature.max()}] mean={target_feature.mean()} std={target_feature.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Before applying a supervised learning algorithm to our data, we usually have to do some preprocessing to get it into a suitable state for training. The most common is to normalise the data. This is so the algorthm gives equal importance to each of the features, which is a usual starting assumption when applying machine learning techniques.\n",
    "\n",
    "Documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we normalise the data, we should split our data into training and test sets. To properly evaulate our regressor performance, we must evaluate it on unseen data to see how well it generalises. We don't want any part of the pipeline to be trained on the data to be used for testing, so we do the split as early as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = sklearn.model_selection.train_test_split(input_features, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, to normalise the input features, we want each feature to have zero mean and standard deviation of 1. The StandardScaler class does this. There are many other preprocessing classes for differents sort of data and different required preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = sklearn.preprocessing.StandardScaler()\n",
    "input_scaler.fit(input_train)\n",
    "X_train = input_scaler.transform(input_train)\n",
    "X_test = input_scaler.transform(input_test)\n",
    "y_train = target_train\n",
    "y_test = target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the mean of each column of data is nearly zero, and the standard deviation is 1. Our features are now ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training regression algorithms\n",
    "\n",
    "The next step is to train the regression algorithm on our data. We are using a linear regression so the training involves calculating the linear coefficients using an ordinary least squares solver, which in the case of scikit-learn is using the scipy implementation of this algorithm.\n",
    "\n",
    "The regressor, as with most steps in a scikit-learn pipeline, have 2 key methods. For a regressor these are fit and predict. Calling `fit()` uses the training data to calculate the parameters of the algorithm to give the best match to the target data. Calling `predict()` then gives a prediction for each of the observations which will be as close as possible to the known answer. One can then apply the algorithm to the unseen test data to determine how well the algorithm generalises for those parameters.\n",
    "\n",
    "Documentation: \n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a regressor object, then train the algorithm (i.e. calculate the parameters, in this case the linear coefficients). The calculated linear coefficients are displayed after fitting. Because we generated the data, we can compare our calculated coefficients to those used for generating the data. They are not an exact match because we added noise to our data to make it more \"real\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor1 = sklearn.linear_model.LinearRegression()\n",
    "linear_regressor1.fit(X_train, y_train)\n",
    "linear_regressor1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor1.coef_ - coef1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our algorithm, we use it to calcuate results for both the observations used in training, i.e. how good our fit is, and also on the unseen observations, to see how well the trained algorithm generalises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_train = linear_regressor1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_test = linear_regressor1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "The last step is to evaluate the performance of the trained algorithms. There are many different metrics available from which one choose based on the nature of the problem. They all have the same interface, where one passes the true target values and the predicted target values and the results are calculated. In this case we are using the root mean squared error to measure the regression fit.\n",
    "\n",
    "Documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train = sklearn.metrics.mean_squared_error(y_train, y_out_train, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_test = sklearn.metrics.mean_squared_error(y_test, y_out_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A non-linear example\n",
    "\n",
    "So far we have looked at an example with quite simple, generated data. Now we look at an example that again uses generated data, but now generated through a non-linear process so we can try other regressions algorithms. This is still not a particularly \"real world\" example, but the algorthms used here are a lot more powerful and would be useful in a real-world problem. What this demonstrates is that, although our regressor is now a lot more powerful, the interfaces for the various bits of our processing pipeline looks exactly the same as for our linear case. \n",
    "\n",
    "Documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nl, target_nl = sklearn.datasets.make_friedman1(n_samples=n_samples, n_features=n_features, noise=noise_std)\n",
    "input_nl_train, input_nl_test, target_nl_train, target_nl_test = sklearn.model_selection.train_test_split(input_nl, target_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nl = sklearn.preprocessing.StandardScaler()\n",
    "scaler_nl.fit(input_nl_train)\n",
    "X_nl_train = scaler_nl.transform(input_nl_train)\n",
    "X_nl_test = scaler_nl.transform(input_nl_test)\n",
    "y_nl_train = target_nl_train\n",
    "y_nl_test = target_nl_test\n",
    "reg_mlp1 = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=[20,20,20], max_iter=10000, tol=1e-5)\n",
    "reg_mlp1.fit(X_nl_train, y_nl_train)\n",
    "y_nl_out_train = reg_mlp1.predict(X_nl_train)\n",
    "y_nl_out_test = reg_mlp1.predict(X_nl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_nl_train = sklearn.metrics.mean_squared_error(y_nl_train, y_nl_out_train, squared=False)\n",
    "err_nl_test = sklearn.metrics.mean_squared_error(y_nl_test, y_nl_out_test,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMS error: training data for non-linear example = {err_nl_train:0.3f}; testing data = {err_nl_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn processing pipeline\n",
    "\n",
    "More complicated real-world examples of classification may involve many more steps. Scikit-learn makes this easier by supplying a pipeline class for linking the steps together. This example will show how the above non-linear example could use a pipeline. In this case we are only using 2 elements in our pipeline, but there could be many more elements, which must all have the fit/predict or fit/tranform standard scikit-learn interface.\n",
    "\n",
    "Documentation:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = sklearn.pipeline.Pipeline(\n",
    "    steps=[('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "           ('neural_net', sklearn.neural_network.MLPRegressor(\n",
    "               hidden_layer_sizes=[20,20,20], max_iter=10000, tol=1e-5))\n",
    "          ]\n",
    ")\n",
    "pipeline1.fit(input_nl_train, target_nl_train)\n",
    "y_pipe_out_train = pipeline1.predict(input_nl_train)\n",
    "y_pipe_out_test = pipeline1.predict(input_nl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_pipe_train = sklearn.metrics.mean_squared_error(target_nl_train, y_pipe_out_train, squared=False)\n",
    "err_pipe_test = sklearn.metrics.mean_squared_error(target_nl_test, y_pipe_out_test,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMS error: training data for pipeline example = {err_pipe_train:0.3f}; testing data = {err_pipe_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Steps\n",
    "\n",
    "In a real project there are further steps we would take to check our results. The first is cross-validation. This involves using several different train/test splits and checking that the results are fairly consistent between different randomly selected splits. This checks that we haven't achieved spuriously good results by an accident of the random train/test split selection.\n",
    "\n",
    "In addition, we would usually do some hyperparamter tuning. Hyperparameters are the parameters of the model that are not adjusted by the standard training process (in scikit-learn, training is what is done by calling the `fit()` function of a classifier or regressor). For example, in a neural network the number of hidden layers and the number of neurons in a hidden layer are hyperparameters. We are likely to get different results when selecting different hyperparameters. Hyperparameter tuning refers to a systematic process of arying the hyperparameters to find a set of hyperparameters that are suitable for the problem being solved.\n",
    "\n",
    "Further reading:\n",
    "* Cross validation - https://scikit-learn.org/stable/modules/cross_validation.html \n",
    "* Hyperparameter tuning - https://scikit-learn.org/stable/modules/grid_search.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-science-cop)",
   "language": "python",
   "name": "data-science-cop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
