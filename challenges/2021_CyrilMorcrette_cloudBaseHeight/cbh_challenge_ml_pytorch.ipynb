{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729aad1-261a-48fa-ae59-986428165ebd",
   "metadata": {},
   "source": [
    "# Training a neural network in PyTorch\n",
    "This notebook demonstrates training a classifier in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4cd0c5-8ca7-47ac-bd69-7c717bc08782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imports\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import iris\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6252f7-1692-43fb-911f-5077f709a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings to be used for the notebook\n",
    "\n",
    "REDUCE_DATA_AMOUNT = True\n",
    "HYPERPARAMETER_TUNING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a123c-373b-4b4a-b195-9be635843400",
   "metadata": {},
   "source": [
    "## Loading in the Cloud Base Height Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e77b6bd-0989-4b9a-b7df-962ec9a2cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find files complete, list of paths: ['/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa004.nc', '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa010.nc', '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa018.nc', '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa022.nc']\n"
     ]
    }
   ],
   "source": [
    "if REDUCE_DATA_AMOUNT:\n",
    "    # hard coded paths for development\n",
    "    paths_to_load = [ '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa004.nc',\n",
    "            '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa010.nc',\n",
    "            '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa018.nc',\n",
    "            '/scratch/hsouth/cbh_data/20160101T0000Z_glm_pa022.nc']\n",
    "else:\n",
    "    paths_to_load = []\n",
    "    # search on cbh_data directory within user scrath space\n",
    "    file_directory = (pathlib.Path(os.environ['SCRATCH']) / 'cbh_data')\n",
    "    for path in os.listdir(file_directory):\n",
    "        if re.search(r'.nc\\b', path):\n",
    "            paths_to_load.append(str((pathlib.Path(os.environ['SCRATCH']) / 'cbh_data') / path))\n",
    "            \n",
    "print('Find files complete, list of paths:', paths_to_load)\n",
    "\n",
    "cubes = iris.load(paths_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fa1e2e-b4ca-4ced-86d1-cb7abbf36ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: m01s05i250 / (unknown)              (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "1: cloud_volume_fraction_in_atmosphere_layer / (1) (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "2: air_pressure / (Pa)                 (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "3: air_temperature / (K)               (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "4: convective_rainfall_flux / (kg m-2 s-1) (time: 4; latitude: 480; longitude: 640)\n",
      "5: convective_snowfall_flux / (kg m-2 s-1) (time: 4; latitude: 480; longitude: 640)\n",
      "6: specific_humidity / (kg kg-1)       (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "7: stratiform_rainfall_flux / (kg m-2 s-1) (time: 4; latitude: 480; longitude: 640)\n",
      "8: stratiform_snowfall_flux / (kg m-2 s-1) (time: 4; latitude: 480; longitude: 640)\n",
      "9: upward_air_velocity / (m s-1)       (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n"
     ]
    }
   ],
   "source": [
    "# show cubes\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da491ed-9be1-41be-9d6a-1ddb7a8089d8",
   "metadata": {},
   "source": [
    "## Preprocess the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf809cd-a16a-4a80-a5f7-6311f89ef1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target cube:\n",
      " 0: cloud_volume_fraction_in_atmosphere_layer / (1) (time: 4; model_level_number: 70; latitude: 480; longitude: 640) \n",
      "\n",
      "input cubes:\n",
      " 0: air_pressure / (Pa)                 (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "1: air_temperature / (K)               (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n",
      "2: specific_humidity / (kg kg-1)       (time: 4; model_level_number: 70; latitude: 480; longitude: 640)\n"
     ]
    }
   ],
   "source": [
    "# extract data we want for the task, temperature, pressure, and humidity for inputs, and cloud volume for outputs\n",
    "list_of_input_cubes = [ 'air_temperature',\n",
    "                         'air_pressure',\n",
    "                         'specific_humidity']\n",
    "target_cube_name = ['cloud_volume_fraction_in_atmosphere_layer']\n",
    "\n",
    "target_cube = iris.cube.CubeList(cube for cube in cubes if (cube.long_name) in target_cube_name)\n",
    "input_cubes = iris.cube.CubeList(cube for cube in cubes if (cube.standard_name) in list_of_input_cubes)\n",
    "\n",
    "# verify success\n",
    "print(\"target cube:\\n\",target_cube, '\\n')\n",
    "print(\"input cubes:\\n\",input_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9feddf51-b706-4221-8e61-0e13a31612a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 70, 480, 640)\n",
      "(1, 4, 70, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Flatten time and lat/long down to sample number\n",
    "# defining function as preprocessing is applied to both input and target\n",
    "def flatten_cubes_with_numpy(cube_list):\n",
    "    cube_array = np.array([cube.data for cube in cube_list])\n",
    "    \n",
    "    # print(\"created dimensions:\", cube_array.shape)\n",
    "    \n",
    "    cube_num, time, height, lat, long = cube_array.shape\n",
    "    \n",
    "    # # verify shape\n",
    "    # print(cube_array.shape)\n",
    "    \n",
    "    # swap axis of time and height to ensure flattening preserves height\n",
    "    cube_array = cube_array.transpose(0,2,1,3,4)\n",
    "    cubes_flattened = np.reshape(cube_array, (cube_num, height,(lat*long*time)))\n",
    "    \n",
    "    # print(\"new dimensions\", cubes_flattened.shape)\n",
    "    \n",
    "    cube_to_return = cubes_flattened.T\n",
    "    # remove unnecessary dimensions\n",
    "    cube_to_return = cube_to_return.squeeze()\n",
    "    return cube_to_return\n",
    "\n",
    "input_array = flatten_cubes_with_numpy(input_cubes)\n",
    "target_array = flatten_cubes_with_numpy(target_cube)\n",
    "\n",
    "# print(\"verify squeeze\", target_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddfa4f-34d0-4f37-b7c6-3f402880b21e",
   "metadata": {},
   "source": [
    "## Preprocess the data toward ML algorithm input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef513328-caa2-400a-aecf-d1ec119f5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cloud bases found: (962499,)\n",
      "Out of samples: 1228800\n"
     ]
    }
   ],
   "source": [
    "# preprocess the target\n",
    "# for the target, we define a cloud exisitng in a height layer:\n",
    "# if the cloud volume fraction is greater than 2 out of possible 8 oktas\n",
    "cloud_threshold = 2./8.\n",
    "#find the first occurences where the cloud volume is greater than the threshold, \n",
    "# stores 0 otherwise\n",
    "cloud_over_threshold = np.where(target_array>cloud_threshold)\n",
    "_, first_duplicate_indicies = np.unique(cloud_over_threshold[0], return_index=True)\n",
    "print(\"Number of cloud bases found:\",first_duplicate_indicies.shape)\n",
    "print(\"Out of samples:\", target_array.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68006d8c-1fd4-45ee-bf6b-d41062687915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of clouds at final height level: (array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "#for clouds where no base was found, add a marker at the final height layer \n",
    "# (where no cloud volume over threshold appears in the data)\n",
    "\n",
    "# verify the claim that no cloud bases appear in the final layer\n",
    "# can be strengthened to, no clouds exist in the final layer\n",
    "print(\"list of clouds at final height level:\", np.where(target_array[:,-1]>cloud_threshold))\n",
    "\n",
    "# encode the cloud in onehot vector\n",
    "one_hot_encoded_bases = np.zeros(target_array.shape)\n",
    "one_hot_encoded_bases[cloud_over_threshold[0][first_duplicate_indicies],cloud_over_threshold[1][first_duplicate_indicies]] = 1\n",
    "# mark the end (final layer) if no cloud base detected\n",
    "flip = lambda booleanVal: not booleanVal\n",
    "vflip = np.vectorize(flip)\n",
    "one_hot_encoded_bases[np.where(vflip(np.any(one_hot_encoded_bases, axis=1)))[0], -1] = 1\n",
    "\n",
    "# Now reduce vectors as if each height layer is treated as a class where the model will predict, onehot -> class label e.g. 0,0,1,0, -> 2\n",
    "class_label_encoded_bases = np.argmax(one_hot_encoded_bases,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe624c8-e933-4cf8-b8a0-ba927853b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1228800, 70, 3)\n"
     ]
    }
   ],
   "source": [
    "# preprocess the inputs\n",
    "# normalize variables (want access to unique features at top-level for processing access,\n",
    "# so transpose, process, and transpose back)\n",
    "data_x = input_array.T\n",
    "data_x = (data_x - data_x.min(axis=(1,2)).reshape((3,1,1))) / (data_x.ptp(axis=(1,2)).reshape((3,1,1)))\n",
    "data_x = data_x.T\n",
    "\n",
    "# # verify dimensions\n",
    "# print(data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c385de7a-6c99-443a-b793-3cef3ff39dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: (1228800, 70, 3)\n",
      "Output dim: (1228800, 70)\n"
     ]
    }
   ],
   "source": [
    "#verify input and output shapes\n",
    "print(\"Input dim:\", data_x.shape)\n",
    "print(\"Output dim:\", one_hot_encoded_bases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f09d7a7-82bb-4163-9eda-55ea7ab25730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dtype float32\n",
      "height encoding dtype float32\n"
     ]
    }
   ],
   "source": [
    "# create an extra positional encoding optionally for input use\n",
    "\n",
    "sample_num, height_dim, _ = data_x.shape\n",
    "# generate height values\n",
    "height_position_vector = np.arange(height_dim)\n",
    "# extend dimensions out to match input feats\n",
    "height_position_vector = np.repeat([height_position_vector], sample_num, axis=0)\n",
    "\n",
    "# # verify\n",
    "# print(height_position_vector.shape)\n",
    "\n",
    "x,y = height_position_vector.shape\n",
    "# add a dimension for height to act as a feature\n",
    "height_position_vector = height_position_vector.reshape(x,y,1)\n",
    "\n",
    "# fit the dtype of the feature to match the dtype of other feats\n",
    "height_position_vector = height_position_vector.astype(data_x.dtype)\n",
    "\n",
    "# # combine height feature into input array \n",
    "# data_x = np.concatenate((height_position_vector, data_x), axis=2, dtype=np.float32) #leave the concat for within the model after producing embedding\n",
    "\n",
    "# verify datatypes\n",
    "print(\"input dtype\", data_x.dtype)\n",
    "print(\"height encoding dtype\", height_position_vector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394ce14-4d59-4c99-aed4-141bae1c412f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get data into training, validation, and testing sets + load data into library specific datastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9889f-b570-4dcf-bd3a-1ed1b1f25379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dffba27b-d62f-4bcd-8132-e668c3479e3b",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14361eb6-2ec1-4256-ab8e-3b57e0441b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab5c4-12e5-48cb-a947-f4f70a0e520e",
   "metadata": {},
   "source": [
    "## Perform the network initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce919a8-22f0-46c1-a895-6b7c07ccdba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba52d2c-1212-4894-ad05-3a9f7761ccf2",
   "metadata": {},
   "source": [
    "## Display and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a375e-8479-496b-87e1-821431729d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
