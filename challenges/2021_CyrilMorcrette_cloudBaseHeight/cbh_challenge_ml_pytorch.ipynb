{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729aad1-261a-48fa-ae59-986428165ebd",
   "metadata": {},
   "source": [
    "# Training a neural network in PyTorch\n",
    "This notebook demonstrates training a classifier in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2ebc23-4958-4cc3-8f6b-c30686f571eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pl\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpython\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "import os\n",
    "import dask\n",
    "import dask.array \n",
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d53e4d-26b1-4d18-a0b7-3f7232f73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_directory = pathlib.Path(os.environ['SCRATCH']) / 'cbh_data'\n",
    "\n",
    "dev_data_path = root_data_directory / 'analysis_ready' / 'dev.zarr' \n",
    "training_data_path = root_data_directory / 'analysis_ready' / 'train.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d753ee3-ace0-41f2-9500-0e5bb764dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "def load_data_from_zarr(path):\n",
    "    \n",
    "    store = zarr.DirectoryStore(training_data_path)\n",
    "    zarr_group = zarr.group(store=store)\n",
    "    print('Loaded zarr, file information:\\n', zarr_group.info, '\\n')\n",
    "    \n",
    "    x = dask.array.from_zarr(zarr_group['humidity_temp_pressure_x.zarr'])\n",
    "    y_lab = dask.array.from_zarr(zarr_group['onehot_cloud_base_height_y.zarr'])\n",
    "    y_cont = dask.array.from_zarr(zarr_group['cloud_volume_fraction_y.zarr'])\n",
    "    \n",
    "    return x, y_lab, y_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96b79fe-a8c4-4026-a77f-39d048642745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_volume_fraction_y.zarr, humidity_temp_pressure_x.zarr,\n",
      "            : onehot_cloud_base_height_y.zarr\n",
      " \n",
      "\n",
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_volume_fraction_y.zarr, humidity_temp_pressure_x.zarr,\n",
      "            : onehot_cloud_base_height_y.zarr\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input, train_labels, train_cloud_volume = load_data_from_zarr(training_data_path)\n",
    "dev_input, dev_labels, dev_cloud_volume = load_data_from_zarr(dev_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffba27b-d62f-4bcd-8132-e668c3479e3b",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14361eb6-2ec1-4256-ab8e-3b57e0441b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RNN\n",
    "class CloudBaseLSTM(pl.LightningModule):\n",
    "    def __init__(self, inputSize, lstmLayers, lstmHiddenSize, output_size, height_dimension, embed_size, BILSTM=True, batch_first=False, lr=2e-3, log_boolean=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.LSTM = torch.nn.LSTM(inputSize+embed_size, lstmHiddenSize, lstmLayers, batch_first=batch_first, bidirectional=BILSTM, proj_size=output_size)\n",
    "        \n",
    "        self.linearCap = torch.nn.Linear(height_dimension, height_dimension)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.proj_size = output_size\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.height_embedding = torch.nn.Embedding(height_dimension, embed_size)\n",
    "        self.BILSTM = BILSTM\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.loss_fn_vol = torch.nn.MSELoss()\n",
    "        self.loss_fn_base = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.log = log_boolean\n",
    "        \n",
    "    def forward(self, x, height):\n",
    "        \n",
    "        #produce height embeds\n",
    "        height_embeds = self.height_embedding(height)\n",
    "        height_embeds = torch.flatten(height_embeds, start_dim=2)\n",
    "        # print(height_embeds.size())\n",
    "        \n",
    "        #concat with feature vector\n",
    "        x_and_height = torch.cat((x, height_embeds), 2)\n",
    "        \n",
    "        #send through LSTM\n",
    "        lstm_out, _ = self.LSTM(x_and_height)\n",
    "        # combine backward and forward LSTM outputs for each cell\n",
    "        if(self.BILSTM):\n",
    "            lstm_out = lstm_out[:,:,:self.proj_size] + lstm_out[:,:,self.proj_size:]\n",
    "        # combinedLSTMOut = combinedLSTMOut / 2\n",
    "        \n",
    "        # # softmax but check for batch first\n",
    "        # softmax_dim = 0\n",
    "        # if self.batch_first:\n",
    "        #     softmax_dim = 1\n",
    "            \n",
    "            \n",
    "        # flatten seq out\n",
    "        lstm_out = torch.flatten(lstm_out, start_dim=1)\n",
    "        \n",
    "        # #normalization\n",
    "        # out = torch.nn.functional.log_softmax(nn_out, dim=softmax_dim)\n",
    "        \n",
    "        # apply ReLU\n",
    "        relu_out = self.relu(lstm_out)\n",
    "        \n",
    "        # apply linear layer for base prediction\n",
    "        nn_out = self.linearCap(relu_out)\n",
    "        \n",
    "        # return both the nn_out and the lstm out for loss calculations\n",
    "        return nn_out, relu_out\n",
    "    \n",
    "    def generic_model_step(self, batch, batch_idx, str_of_step_name):\n",
    "        \n",
    "         #### #### #### WARNING MAY CAUSE SOME WEIRD OBJECT ORIENTED RELATED BEHAVIOUR I AM UNAWARE ABOUT AND NOT WORK #### #### ####\n",
    "            \n",
    "        base_pred, vol_pred = self(batch['x'], batch['height_vector'])\n",
    "        loss_1 = self.loss_fn_vol(vol_pred, batch['cloud_volume_target'])\n",
    "        loss_2 = self.loss_fn_base(base_pred, batch['cloud_base_target'])\n",
    "        loss = (loss_1*40) + loss_2\n",
    "        \n",
    "        #log to tensorboard\n",
    "        self.log(str_of_step_name, 'loss', loss)\n",
    "        self.log(str_of_step_name, 'volume loss component', loss_1)\n",
    "        self.log(str_of_step_name, 'base height loss component', loss_2)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        return self.generic_model_step(batch, batch_idx, 'training')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        return self.generic_model_step(batch, batch_idx, 'validation')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        return self.generic_model_step(batch, batch_idx, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        return optim\n",
    "\n",
    "# define torch dataloader\n",
    "class CBH_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y, cloud_base_label):\n",
    "        print('begin init')\n",
    "        \n",
    "        self.temp_humidity_pressure = data_x\n",
    "        self.cloudbase_target = data_y\n",
    "        self.cbh_label = cloud_base_label\n",
    "        \n",
    "        self.height_layer_number = data_x.shape[1] # take the shape at index 1 as data_x of format sample, height, feature\n",
    "        \n",
    "        assert self.height_layer_number == 70\n",
    "        \n",
    "        # legacy cloud base height\n",
    "        # self.height_encoding = torch.from_numpy(data_height)\n",
    "        print('end init')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.temp_humidity_pressure)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # since dask is being used, first compute the values on the index given to the get function, convert the array to tensor for pytorch\n",
    "        \n",
    "        input_features = torch.from_numpy(self.temp_humidity_pressure[idx].compute())\n",
    "        output_target = torch.from_numpy(self.cloudbase_target[idx].compute())\n",
    "        output_target = output_target.type(torch.FloatTensor)\n",
    "        cbh_lab = torch.from_numpy(self.cbh_label[idx].compute())\n",
    "        \n",
    "        print('CALL ON GETITEM')\n",
    "        \n",
    "        height_vec = torch.from_numpy(np.arange(self.height_layer_number)) # should have produced this vector here, as it is the same every time, but will leave it since sunken cost and maybe it improves performance??? \n",
    "        \n",
    "        item_in_dataset = {'x':input_features, 'cloud_volume_target':output_target, 'cloud_base_target':cbh_lab, 'height_vector':height_vec}\n",
    "        return item_in_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b42189-6eff-4824-8ffd-29e4caf4ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dask specific collate function for dataloader, collate is the step where the dataloader combines all the samples into a singular batch to be enumerated on, \n",
    "# after getting all items \n",
    "\n",
    "def dataloader_collate_with_dask(batch):\n",
    "    \n",
    "    assert torch.utils.data.get_worker_info() is None # if this assertion fails, there are issues in code and this case needs to be handled see pytorch source of default collate fn\n",
    "    print(batch)\n",
    "    return 1\n",
    "    \n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum(x.numel() for x in batch)\n",
    "            storage = elem.storage()._new_shared(numel, device=elem.device)\n",
    "            out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return default_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        try:\n",
    "            return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n",
    "        except TypeError:\n",
    "            # The mapping type may not support `__init__(iterable)`.\n",
    "            return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
    "\n",
    "        if isinstance(elem, tuple):\n",
    "            return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
    "        else:\n",
    "            try:\n",
    "                return elem_type([default_collate(samples) for samples in transposed])\n",
    "            except TypeError:\n",
    "                # The sequence type may not support `__init__(iterable)` (e.g., `range`).\n",
    "                return [default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83e7351-b657-4e9b-8997-2d4022f8b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enforce reproducibility\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab5c4-12e5-48cb-a947-f4f70a0e520e",
   "metadata": {},
   "source": [
    "## Perform the network initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce919a8-22f0-46c1-a895-6b7c07ccdba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin init\n",
      "end init\n",
      "begin init\n",
      "end init\n"
     ]
    }
   ],
   "source": [
    "# load into torcg dataset \n",
    "\n",
    "collate_fn = dataloader_collate_with_dask\n",
    "\n",
    "train_cbh_data = CBH_Dataset(train_input, train_cloud_volume, train_labels)\n",
    "dev_cbh_data = CBH_Dataset(dev_input, dev_cloud_volume, dev_labels)\n",
    "\n",
    "height_dim = train_input.shape[1]\n",
    "\n",
    "# define model and hyperparameters\n",
    "layers = 3\n",
    "input_size = train_input.shape[2] # input size is the cell input (feat dim)\n",
    "output_size = 1 # for each height layer, predict one value for cloud base prob\n",
    "hidden_size = 32\n",
    "embed_size = 5\n",
    "BILSTM = False\n",
    "batch_first = True\n",
    "\n",
    "learn_rate = 0.002\n",
    "\n",
    "log_with_pl = False # do not log, as track with mlFlow\n",
    "\n",
    "model = CloudBaseLSTM(input_size, layers, hidden_size, output_size, height_dim, embed_size, BILSTM, batch_first, lr=learn_rate, log_boolean=log_with_pl)\n",
    "\n",
    "# define training related hyperparameters\n",
    "\n",
    "epochs = 10\n",
    "max_time =\"00:12:00:00\" #dd:hh:mm:ss\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# after training parameters defined, load datasets into dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_cbh_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dev_cbh_data, batch_size=batch_size, collate_fn=collate_fn) # don't shuffle in val\n",
    "\n",
    "# define trainer\n",
    "\n",
    "# \n",
    "trainer = pl.Trainer(max_epochs = epochs, deterministic=True, check_val_every_n_epoch=1, devices=\"auto\", accelerator=\"auto\", max_time=max_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39551462-62c9-4181-a41b-640463f4196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mlflow logging\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2dade6-a531-43b9-a88c-dcfb92937ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /net/home/h02/hsouth/github_committing/data_science_cop/challenges/2021_CyrilMorcrette_cloudBaseHeight/lightning_logs\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | LSTM             | LSTM             | 2.5 K \n",
      "1 | linearCap        | Linear           | 5.0 K \n",
      "2 | relu             | ReLU             | 0     \n",
      "3 | height_embedding | Embedding        | 350   \n",
      "4 | loss_fn_vol      | MSELoss          | 0     \n",
      "5 | loss_fn_base     | CrossEntropyLoss | 0     \n",
      "------------------------------------------------------\n",
      "7.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.8 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d35bda523f3448db9348f78fb799216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h02/hsouth/.conda/envs/pl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:495: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/h02/hsouth/.conda/envs/pl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n",
      "CALL ON GETITEM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h02/hsouth/.conda/envs/pl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "2022/08/08 13:51:46 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: ('{} cannot be pickled', '_SingleProcessDataLoaderIter')\n"
     ]
    }
   ],
   "source": [
    "# run the training function \n",
    "with mlflow.start_run() as run:\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba52d2c-1212-4894-ad05-3a9f7761ccf2",
   "metadata": {},
   "source": [
    "## Display and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108cb0a-4886-40ac-8ab8-af913c5d3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display mlflow output\n",
    "print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a375e-8479-496b-87e1-821431729d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some predictions\n",
    "#have a look at model predictions for a sample after training\n",
    "# {'x':input_features, 'cloud_volume_target':output_target, 'cloud_base_target':cbh_lab, 'height_vector':height_vec}\n",
    "\n",
    "sample = cbh_data[1:2]\n",
    "\n",
    "modelOutBase, moVol = model(sample['x'], sample['height_vector'])\n",
    "print(modelOutBase)\n",
    "print(sample['cloud_base_target'])\n",
    "print('')\n",
    "print(moVol)\n",
    "print(sample['cloud_volume_target'])\n",
    "\n",
    "\n",
    "# predictionInit = torch.zeros(modelOut.size(1))\n",
    "# predictionInit[torch.argmax(modelOut,dim=1)] = 1\n",
    "# print(predictionInit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
