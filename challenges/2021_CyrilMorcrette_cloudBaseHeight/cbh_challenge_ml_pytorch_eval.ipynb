{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c347a5-7180-4afc-b334-9838147b748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbh_torch_lstm # defined in directory (model definition)\n",
    "import cbh_data_definitions\n",
    "import zarr\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc99b1-83f8-4a29-86ce-f73fcf802b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b1c924-d784-4214-a3a8-1302f2dcf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_directory = pathlib.Path(os.environ['SCRATCH']) / 'cbh_data'\n",
    "test_data_path = root_data_directory / 'analysis_ready' / 'dev.zarr' # must change with available test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3263ca16-6caa-442b-a1f5-b26c2428b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_volume_fraction_y.zarr, humidity_temp_pressure_x.zarr,\n",
      "            : onehot_cloud_base_height_y.zarr\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input, test_labels, test_cloud_volume = cbh_data_definitions.load_data_from_zarr(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb955ac-703a-4cf2-ac65-c7313f3ae2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = cbh_data_definitions.dataloader_collate_with_dask\n",
    "batch_size = 1000\n",
    "workers_on_system = 0\n",
    "test_dataloader = cbh_data_definitions.define_data_get_loader(test_input,\n",
    "                                                              test_cloud_volume,\n",
    "                                                              test_labels, \n",
    "                                                              batch_size=batch_size, \n",
    "                                                              shuffle=False, \n",
    "                                                              num_workers = workers_on_system, \n",
    "                                                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffba27b-d62f-4bcd-8132-e668c3479e3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69566bef-8229-4dd2-b746-82e8737de679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('final_out_lab.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6a3ef9-7cdf-47e6-aa1b-53321d5c2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 3\n",
    "input_size = test_input.shape[2] # input size is the cell input (feat dim)\n",
    "output_size = 1 # for each height layer, predict one value for cloud base prob\n",
    "hidden_size = 32\n",
    "embed_size = 3\n",
    "BILSTM = True\n",
    "batch_first = True\n",
    "learn_rate = 0.001\n",
    "height_dim = test_input.shape[1]\n",
    "\n",
    "model = cbh_torch_lstm.CloudBaseLSTM(input_size, layers, hidden_size, output_size, height_dim, embed_size)\n",
    "model.state_dict = checkpoint['state_dict']\n",
    "\n",
    "# model = cbh_torch_lstm.CloudBaseLSTM.load_from_checkpoint('final_out_lab.ckpt', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8e70f-2968-4caf-b833-5fdf75a40b56",
   "metadata": {},
   "source": [
    "## Get model predictions for Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43319b26-9e48-426f-a840-766ba11f7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 70)\n",
      "(1000, 70)\n"
     ]
    }
   ],
   "source": [
    "all_preds = np.empty((0,70))\n",
    "all_targs = np.empty((0,70))\n",
    "for i, sample_batch in enumerate(test_dataloader):\n",
    "    if i > 0: break # temp get only the first batch\n",
    "    all_targs = np.concatenate((all_targs, sample_batch['cloud_base_target']), axis=0)\n",
    "    height = sample_batch['height_vector']\n",
    "    x = sample_batch['x']\n",
    "    batch_preds = model(x, height) # self call = forward\n",
    "    all_preds = np.concatenate((all_preds,batch_preds[0].detach().numpy()), axis=0) # get second value only (since model doesn't support the other return per how it is currently defined)\n",
    "\n",
    "print(all_targs.shape)\n",
    "print(all_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ad20a18-1170-4da1-91a6-dae7b2c2df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02812776 -0.07781146  0.0626948  -0.08563723 -0.11669876 -0.10598429\n",
      "  0.03966087 -0.09225918 -0.04651425  0.1156247   0.11421951  0.07058091\n",
      " -0.0844821   0.08742284 -0.10865832 -0.09843732  0.06928282  0.0083699\n",
      "  0.01322776 -0.02690658 -0.0898084  -0.08008031  0.04124816  0.06330191\n",
      " -0.09607642  0.00219133  0.08416506  0.02324097  0.06305565  0.05053411\n",
      "  0.11220004  0.10415665  0.03736757  0.10131148 -0.06242672  0.10829218\n",
      " -0.10935006  0.04064533  0.0276012   0.06404807 -0.05122092  0.06662233\n",
      " -0.05878032  0.0359249  -0.05011643 -0.08382751 -0.10241412  0.07698797\n",
      " -0.08534922 -0.07265964  0.03688104 -0.08927614  0.02857562 -0.03174012\n",
      " -0.00939858 -0.03466484  0.09406193 -0.09890064  0.01117128  0.09528607\n",
      " -0.0998457  -0.06829891  0.03459731  0.01586575 -0.05767401 -0.03828694\n",
      " -0.06396797 -0.05973816 -0.12091554  0.10869594]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# view my model's gross abuse of generalized data statistics\n",
    "print(all_preds[7])\n",
    "print(all_targs[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59b07d-7da2-442b-ad38-acd6f20ec2f5",
   "metadata": {},
   "source": [
    "### convert model outs to the task (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3768c836-5583-47d8-aa4a-278f4a0f82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "16\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "def calc_cloudbase_return_both_class_lab_and_onehot(cloud_vol_array):\n",
    "    cloud_threshold = 2./8.\n",
    "    cloud_over_threshold = np.where(cloud_vol_array>cloud_threshold)\n",
    "    sample_with_cloud = cloud_over_threshold[0]\n",
    "    index_on_sample = cloud_over_threshold[1]\n",
    "    _, first_duplicate_indicies = np.unique(sample_with_cloud, return_index=True)\n",
    "    # encode the cloud in onehot vector\n",
    "    one_hot_encoded_bases = np.zeros(cloud_vol_array.shape)\n",
    "    one_hot_encoded_bases[sample_with_cloud[first_duplicate_indicies],index_on_sample[first_duplicate_indicies]] = 1\n",
    "    # mark the end (final layer) if no cloud base detected\n",
    "    flip = lambda booleanVal: not booleanVal\n",
    "    vflip = np.vectorize(flip)\n",
    "    one_hot_encoded_bases[np.where(vflip(np.any(one_hot_encoded_bases, axis=1)))[0], -1] = 1\n",
    "    # Now reduce vectors as if each height layer is treated as a class where the model will predict, onehot -> class label e.g. 0,0,1,0, -> 2\n",
    "    class_label_encoded_bases = np.argmax(one_hot_encoded_bases, axis=1)\n",
    "    \n",
    "    return one_hot_encoded_bases, class_label_encoded_bases\n",
    "\n",
    "def calc_lab(tar_lab, pred_lab):\n",
    "    return np.argmax(tar_lab, axis=1), np.argmax(pred_lab, axis=1)\n",
    "    \n",
    "# tar_onehot, tar_lab = calc_cloudbase_return_both_class_lab_and_onehot(all_targs)\n",
    "# pred_onehot, pred_lab = calc_cloudbase_return_both_class_lab_and_onehot(all_preds)\n",
    "\n",
    "tar_lab, pred_lab = calc_lab(all_targs, all_preds)\n",
    "\n",
    "# print(tar_onehot[0])\n",
    "print(all_targs[0])\n",
    "print(tar_lab[0])\n",
    "print(pred_lab\n",
    "print(tar_lab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b70551-8b3e-4b9d-8082-91aa65c59844",
   "metadata": {},
   "source": [
    "## Evaluate model returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f599537e-3d3a-411e-8ff8-0295211326a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_cbh_eval(preds, targs):\n",
    "    # for all tested height layers\n",
    "    abcd_list = []\n",
    "    for k in np.arange(0,53,1):\n",
    "            a=0.0\n",
    "            b=0.0\n",
    "            c=0.0\n",
    "            d=0.0\n",
    "            for i in np.arange(0,targs.shape[1]):\n",
    "                # Is the cloud-base at this level or below.\n",
    "                if   targs[0,i]<=np.float64(k) and preds[0,i] <=np.float64(k):\n",
    "                    # Hit\n",
    "                    a=a+1.0\n",
    "                elif targs[0,i]> np.float64(k) and preds[0,i] <=np.float64(k):\n",
    "                    # False alarm\n",
    "                    b=b+1.0\n",
    "                elif targs[0,i]<=np.float64(k) and preds[0,i] > np.float64(k):\n",
    "                    # Miss\n",
    "                    c=c+1.0\n",
    "                else:\n",
    "                    # Correct negative\n",
    "                    d=d+1.0\n",
    "            # if a == 0.0 or b == 0.0:\n",
    "            #     a += 1 \n",
    "            #     b += 1\n",
    "            abcd_list.append((a,b,c,d))\n",
    "    return abcd_list\n",
    "\n",
    "import typing\n",
    "# new function\n",
    "def new_cbh_eval(preds: typing.Iterable, targs: typing.Iterable, test_up_to_layer=53) -> (list[int], list[int]):\n",
    "    abcd_list = []\n",
    "    preds = preds[:test_up_to_layer]\n",
    "    targs = targs[:test_up_to_layer]\n",
    "    print(np.where(preds<=targs))\n",
    "    # a =\n",
    "    \n",
    "    return\n",
    "    return abcd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422082d3-2932-49bb-9d48-dc1dbeba4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52]),)\n"
     ]
    }
   ],
   "source": [
    "new_cbh_eval(pred_lab, tar_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399f6b11-89ea-4a85-9336-dca2dbba05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "rearange_for_old_eval_target = np.expand_dims(tar_lab, 0)\n",
    "rearange_for_old_eval_pred = np.expand_dims(pred_lab, 0)\n",
    "abcd_old = old_cbh_eval(rearange_for_old_eval_pred, rearange_for_old_eval_target)\n",
    "assert all([x in [1000,1002] for x in list(map(sum, abcd_old))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe80cd6-45e6-48d9-b0c9-2365462b5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (54.0, 946.0, 0.0, 0.0), (77.0, 923.0, 0.0, 0.0), (852.0, 148.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (860.0, 140.0, 0.0, 0.0), (966.0, 34.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0), (1000.0, 0.0, 0.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(abcd_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ee3f3-3798-4075-94a5-1cf4e7427057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(tar_lab, return_counts=True))\n",
    "print(np.unique(pred_lab, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f7929e4-0713-4385-955e-0e9039dba890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 correct\n",
      "5.4%\n"
     ]
    }
   ],
   "source": [
    "# check accuracy for sanity\n",
    "correct_preds = tar_lab==pred_lab\n",
    "correct = np.sum(correct_preds)\n",
    "print(correct, 'correct')\n",
    "acc = 100 * (correct / len(tar_lab))\n",
    "print(str(acc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62ce09-e223-4b7d-a2ff-89e5eaaea53f",
   "metadata": {},
   "source": [
    "## Present Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d934b-182d-407c-97ce-f94684329c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-pl Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-pl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
