{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6729aad1-261a-48fa-ae59-986428165ebd",
   "metadata": {},
   "source": [
    "# Training a neural network in PyTorch\n",
    "\n",
    "This notebook demonstrates training a classifier in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2ebc23-4958-4cc3-8f6b-c30686f571eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl ver: 1.6.4\n"
     ]
    }
   ],
   "source": [
    "# file handling\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import dask\n",
    "import dask.array\n",
    "\n",
    "# math operators\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# ml\n",
    "import torch\n",
    "import zarr\n",
    "\n",
    "print(\"pl ver:\", pl.__version__)\n",
    "\n",
    "import datetime\n",
    "\n",
    "# training helpers\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    RichProgressBar,\n",
    ")  # this progress bar works through jupyterHub on spice\n",
    "\n",
    "# defined in directory (model related definitions)\n",
    "import cbh_data_definitions\n",
    "import cbh_torch_lstm\n",
    "import cbh_torch_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508614de-9142-42df-811b-bba77c76bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_PACKAGES = True\n",
    "if RELOAD_PACKAGES:\n",
    "    import importlib\n",
    "    importlib.reload(cbh_torch_lstm)\n",
    "    importlib.reload(cbh_torch_MLP)\n",
    "    importlib.reload(cbh_data_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d53e4d-26b1-4d18-a0b7-3f7232f73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_directory = pathlib.Path(os.environ[\"SCRATCH\"]) / \"cbh_data\"\n",
    "\n",
    "dev_data_path = root_data_directory / \"analysis_ready\" / \"dev.zarr\"\n",
    "training_data_path = root_data_directory / \"analysis_ready\" / \"train.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c96b79fe-a8c4-4026-a77f-39d048642745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_base_label_y.zarr, cloud_volume_fraction_y.zarr,\n",
      "            : humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n",
      "Loaded zarr, file information:\n",
      " Name        : /\n",
      "Type        : zarr.hierarchy.Group\n",
      "Read-only   : False\n",
      "Store type  : zarr.storage.DirectoryStore\n",
      "No. members : 3\n",
      "No. arrays  : 3\n",
      "No. groups  : 0\n",
      "Arrays      : cloud_base_label_y.zarr, cloud_volume_fraction_y.zarr,\n",
      "            : humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_input,\n",
    "    train_labels,\n",
    "    train_cloud_volume,\n",
    ") = cbh_data_definitions.load_data_from_zarr(training_data_path)\n",
    "\n",
    "(\n",
    "    dev_input, \n",
    "    dev_labels, \n",
    "    dev_cloud_volume \n",
    ") = cbh_data_definitions.load_data_from_zarr(dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c8b8915-497a-4d90-8ec2-971caa896dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_DATA = True\n",
    "LIMIT_DATA_INT = 502400\n",
    "if LIMIT_DATA:\n",
    "    train_input = train_input[:LIMIT_DATA_INT]\n",
    "    train_labels = train_labels[:LIMIT_DATA_INT]\n",
    "    train_cloud_volume = train_cloud_volume[:LIMIT_DATA_INT]\n",
    "    dev_input = dev_input[:LIMIT_DATA_INT]\n",
    "    dev_labels = dev_labels[:LIMIT_DATA_INT]\n",
    "    dev_cloud_volume = dev_cloud_volume[:LIMIT_DATA_INT]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffba27b-d62f-4bcd-8132-e668c3479e3b",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83e7351-b657-4e9b-8997-2d4022f8b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enforce reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab5c4-12e5-48cb-a947-f4f70a0e520e",
   "metadata": {},
   "source": [
    "## Perform the network initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ce919a8-22f0-46c1-a895-6b7c07ccdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and hyperparameters\n",
    "model_hyperparameter_dictionary = {\n",
    "    'LSTM' : {\n",
    "        'input_size' : train_input.shape[2],  # input size is the cell input (feat dim)\n",
    "        'lstm_layers' : 1,\n",
    "        'lstm_hidden_size' : 8,\n",
    "        'output_size' : 1,  # for each height layer, predict one value for cloud base prob\n",
    "        'height_dimension' : train_input.shape[1],\n",
    "        'embed_size' : 1,\n",
    "        'BILSTM' : False,\n",
    "        'batch_first' : True,\n",
    "        'lr' : 0.003,\n",
    "    },\n",
    "    'MLP' : {\n",
    "        'input_size' : train_input.shape[2] * train_input.shape[1],\n",
    "        'ff_nodes' : 256,\n",
    "        'output_size' : train_input.shape[1],\n",
    "        'lr' : 1.0e-4,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_definition_dictionary = {\n",
    "    'LSTM' : cbh_torch_lstm.CloudBaseLSTM(**model_hyperparameter_dictionary['LSTM']),\n",
    "    'MLP' : cbh_torch_MLP.CloudBaseMLP(**model_hyperparameter_dictionary['MLP']),\n",
    "}\n",
    "\n",
    "model = model_definition_dictionary['LSTM'] # pick a model\n",
    "\n",
    "# define training related hyperparameters\n",
    "\n",
    "epochs = 1\n",
    "max_time = \"00:02:20:00\"  # dd:hh:mm:ss\n",
    "\n",
    "# after training parameters defined, load datasets into dataloaders (enforce 0 as workers on sys to prevent multiple packages\n",
    "# trying to parallelise while not communicating\n",
    "workers_on_system = 0\n",
    "collate_fn = cbh_data_definitions.dataloader_collate_with_dask\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adc82bd5-20a8-4072-a266-4c162e41ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing x...\n",
      "Computing y...\n",
      "Computing lab...\n",
      "Computing x...\n",
      "Computing y...\n",
      "Computing lab...\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = None, None\n",
    "\n",
    "INTO_MEMORY = True\n",
    "if INTO_MEMORY:\n",
    "    train_loader = cbh_data_definitions.define_data_get_loader_into_memory(\n",
    "        train_input,\n",
    "        train_cloud_volume,\n",
    "        train_labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers_on_system,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = cbh_data_definitions.define_data_get_loader_into_memory(\n",
    "        dev_input,\n",
    "        dev_cloud_volume,\n",
    "        dev_labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers_on_system,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    train_loader = cbh_data_definitions.define_data_get_loader(\n",
    "        train_input,\n",
    "        train_cloud_volume,\n",
    "        train_labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers_on_system,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = cbh_data_definitions.define_data_get_loader(\n",
    "        dev_input,\n",
    "        dev_cloud_volume,\n",
    "        dev_labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers_on_system,\n",
    "        collate_fn=collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2763eaf2-bedd-4834-a1f2-030b170d1899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# define trainer\n",
    "\n",
    "time_for_checkpoint = datetime.timedelta(minutes=20)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    train_time_interval=time_for_checkpoint\n",
    ")\n",
    "callbacks = [checkpoint_callback, RichProgressBar()]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    deterministic=True,\n",
    "    check_val_every_n_epoch=1,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"auto\",\n",
    "    max_time=max_time,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39551462-62c9-4181-a41b-640463f4196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup mlflow logging\n",
    "\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c2dade6-a531-43b9-a88c-dcfb92937ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0   </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1144/5207</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:04:54 • 0:17:48</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">3.80it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 2.81 v_num: 8837248 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0   \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1144/5207\u001b[0m \u001b[38;5;245m0:04:54 • 0:17:48\u001b[0m \u001b[38;5;249m3.80it/s\u001b[0m \u001b[37mloss: 2.81 v_num: 8837248 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mOverflowError\u001b[0m: serializing a string larger than 4 GiB requires pickle protocol 4 or higher",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run the training function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run() \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:555\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m     patch_function\u001b[38;5;241m.\u001b[39mcall(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m try_log_autologging_event(\n\u001b[1;32m    560\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    561\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     kwargs,\n\u001b[1;32m    566\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/utils/autologging_utils/safety.py:254\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/pytorch/_pytorch_autolog.py:392\u001b[0m, in \u001b[0;36mpatched_fit\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[1;32m    389\u001b[0m     registered_model_name \u001b[38;5;241m=\u001b[39m get_autologging_config(\n\u001b[1;32m    390\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mFLAVOR_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistered_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[0;32m--> 392\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stop_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path:\n\u001b[1;32m    399\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mlog_artifact(\n\u001b[1;32m    400\u001b[0m             local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path,\n\u001b[1;32m    401\u001b[0m             artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestored_model_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/pytorch/__init__.py:306\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    PyTorch logged models\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m pickle_module \u001b[38;5;241m=\u001b[39m pickle_module \u001b[38;5;129;01mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequirements_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequirements_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/models/model.py:294\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m run_id \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mfluent\u001b[38;5;241m.\u001b[39m_get_or_start_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[1;32m    293\u001b[0m mlflow_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(artifact_path\u001b[38;5;241m=\u001b[39martifact_path, run_id\u001b[38;5;241m=\u001b[39mrun_id)\n\u001b[0;32m--> 294\u001b[0m \u001b[43mflavor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mfluent\u001b[38;5;241m.\u001b[39mlog_artifacts(local_path, artifact_path)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/mlflow/pytorch/__init__.py:512\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, requirements_file, extra_files, pip_requirements, extra_pip_requirements, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule\u001b[38;5;241m.\u001b[39msave(pytorch_model, model_path)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m torchserve_artifacts_config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_files:\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/torch/serialization.py:484\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    482\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    483\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 484\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    486\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m~/.conda/envs/pl/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run the training function\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba52d2c-1212-4894-ad05-3a9f7761ccf2",
   "metadata": {},
   "source": [
    "## Display and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "888b7d7e-5158-42a9-b3f2-046dca3a218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auto_logged_info(r):\n",
    "\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
    "    print(\"run_id: {}\".format(r.info.run_id))\n",
    "    print(\"artifacts: {}\".format(artifacts))\n",
    "    print(\"params: {}\".format(r.data.params))\n",
    "    print(\"metrics: {}\".format(r.data.metrics))\n",
    "    print(\"tags: {}\".format(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c108cb0a-4886-40ac-8ab8-af913c5d3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 58af654a13dc4baba74ba7547a9dd44f\n",
      "artifacts: []\n",
      "params: {'epochs': '1', 'optimizer_name': 'Adam', 'lr': '0.003', 'betas': '(0.9, 0.999)', 'eps': '1e-08', 'weight_decay': '0', 'amsgrad': 'False'}\n",
      "metrics: {}\n",
      "tags: {'Mode': 'training'}\n"
     ]
    }
   ],
   "source": [
    "# display mlflow output\n",
    "print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7a9ef9b-ef4f-4c02-9b9f-23fa53a1b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_save_str_element = str(datetime.datetime.now())\n",
    "save_str = \"model_out\" + \"_\" + unique_save_str_element + '.ckpt'\n",
    "trainer.save_checkpoint(save_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ce386db-e33d-42aa-90ff-8a4a4dd3be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 70, 3]) inp\n",
      "torch.Size([1024, 70])\n",
      "torch.Size([1024, 70])  height\n",
      "torch.Size([1024, 70]) preds\n",
      "(1024,) pred label shape\n",
      "(1024,) targ shape\n",
      "378\n",
      "1024\n",
      "36.9140625 %\n"
     ]
    }
   ],
   "source": [
    "# test model functionality\n",
    "example_batch = next(iter(train_loader))\n",
    "heights = example_batch['height_vector']\n",
    "inputs = example_batch['x']\n",
    "print(inputs.shape, 'Input shape')\n",
    "print(\"Height vector shape:\", example_batch['height_vector'].shape)\n",
    "if TRAIN_LSTM:\n",
    "    preds, _ = model(inputs, heights)\n",
    "else:\n",
    "    print(example_batch['x'].shape, 'inp pre-flat')\n",
    "    inputs = torch.flatten(example_batch['x'], start_dim=1)\n",
    "    preds = model(inputs)\n",
    "print(preds.shape, 'prediction output')\n",
    "pred_label = np.argmax(preds.detach().numpy(), axis=1)\n",
    "print(pred_label.shape, 'prediction label shape')\n",
    "targs = example_batch['cloud_base_target']\n",
    "targs = np.array(targs)\n",
    "print(targs.shape, \"targ shape\")\n",
    "correct = targs == pred_label\n",
    "print(\"Correct samples:\", np.count_nonzero(correct))\n",
    "print(\"Total samples tested:\", len(correct))\n",
    "print(\"Accuracy:\", (np.count_nonzero(correct) / len(correct) *100), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
