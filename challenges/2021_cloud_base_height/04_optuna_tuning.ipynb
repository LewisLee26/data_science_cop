{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a7bbf4-00f4-4b5d-97a9-d6d9138d0f8a",
   "metadata": {},
   "source": [
    "Make initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97e418c-c396-4b0d-9157-8f201bb9d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbh_data_definitions\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import mlflow\n",
    "from ray import tune\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.integration.mlflow import mlflow_mixin\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from pytorch_lightning.callbacks import (\n",
    "    RichProgressBar,\n",
    ")\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cbh_torch_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16443e15-8512-4a3f-a3eb-222764cd03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize some settings: mlflow, data directory, resources\n",
    "root_data_directory = pathlib.Path(os.environ[\"SCRATCH\"]) / \"cbh_data\"\n",
    "\n",
    "dev_data_path = root_data_directory / \"analysis_ready\" / \"dev_randomized.zarr\"\n",
    "training_data_path = root_data_directory / \"analysis_ready\" / \"train_randomized.zarr\"\n",
    "\n",
    "mlflow_command_line_run = \"\"\"\n",
    "    mlflow server --port 5001 --backend-store-uri sqlite:///mlflowSQLserver.db  --default-artifact-root ./mlflow_artifacts/\n",
    "\"\"\"\n",
    "mlflow_server_address = 'vld425'\n",
    "mlflow_server_port = 5001\n",
    "mlflow_server_uri = f'http://{mlflow_server_address}:{mlflow_server_port:d}'\n",
    "mlflow_artifact_root = pathlib.Path('./mlflow_artifacts/')\n",
    "\n",
    "hparams_for_mlflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14327c-6a24-4cb2-93da-f2697a78be1e",
   "metadata": {},
   "source": [
    "redefine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cba65e-0d9a-4924-93fb-47b4aacc9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n",
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 87.48 GiB </td>\n",
       "                        <td> 1.82 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (111820800, 70, 3) </td>\n",
       "                        <td> (2329600, 70, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Graph Layers </td>\n",
       "                        <td> 48 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"28\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"32\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"35\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"32\" y2=\"47\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"50\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"54\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"57\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"46\" y2=\"62\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"49\" y2=\"65\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"54\" y2=\"69\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"57\" y2=\"72\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"76\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"64\" y2=\"79\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"84\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"71\" y2=\"87\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"76\" y2=\"91\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"38\" y2=\"2\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"42\" y2=\"7\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"45\" y2=\"10\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"50\" y2=\"14\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"57\" y2=\"22\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"64\" y2=\"29\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"67\" y2=\"32\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"72\" y2=\"36\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"75\" y2=\"39\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"79\" y2=\"44\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"82\" y2=\"47\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"86\" y2=\"51\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"89\" y2=\"54\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"94\" y2=\"58\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"97\" y2=\"61\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"101\" y2=\"66\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">70</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">111820800</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(111820800, 70, 3), dtype=float32, chunksize=(2329600, 70, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init data\n",
    "(\n",
    "    train_input,\n",
    "    train_labels,\n",
    "    _,\n",
    ") = cbh_data_definitions.load_data_from_zarr(training_data_path)\n",
    "\n",
    "(\n",
    "    dev_input, \n",
    "    dev_labels, \n",
    "    _\n",
    ") = cbh_data_definitions.load_data_from_zarr(dev_data_path)\n",
    "\n",
    "# the cloud volume is not needed for the task, so isn't saved on the load\n",
    "# show a chunk\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edeea09a-8bd8-4313-a7ea-7fd8d37bf4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors of chunk:  [1, 2, 4, 5, 7, 8, 10, 13, 14, 16, 20, 25, 26, 28, 32, 35, 40, 50, 52, 56, 64, 65, 70, 80, 91, 100, 104, 112, 128, 130, 140, 160, 175, 182, 200, 208, 224, 256, 260, 280, 320, 325, 350, 364, 400, 416, 448, 455, 512, 520, 560, 640, 650, 700, 728, 800, 832, 896, 910, 1024, 1040, 1120, 1280, 1300, 1400, 1456, 1600, 1664, 1792, 1820, 2080, 2240, 2275, 2560, 2600, 2800, 2912, 3200, 3328, 3584, 3640, 4160, 4480, 4550, 5120, 5200, 5600, 5824, 6400, 6656, 7168, 7280, 8320, 8960, 9100, 10400, 11200, 11648, 12800, 13312, 14560, 16640, 17920, 18200, 20800, 22400, 23296, 25600, 29120, 33280, 35840, 36400, 41600, 44800, 46592, 58240, 66560, 72800, 83200, 89600, 93184, 116480, 145600, 166400, 179200, 232960, 291200, 332800, 465920, 582400, 1164800, 2329600]\n"
     ]
    }
   ],
   "source": [
    "# limit the data by a factor for less data in a tuning trial\n",
    "factors_of_chunk = [n for n in range(1, train_input.chunksize[0] + 1) if train_input.chunksize[0] % n == 0]\n",
    "print(\"Factors of chunk: \", factors_of_chunk)\n",
    "hparams_for_mlflow['Limited sample number'] =  -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbae1c6-5501-40e7-a2ba-4b1ef270f170",
   "metadata": {},
   "source": [
    "setup study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bd6235-7bfe-4bd1-aa06-ae1f5f86c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE ALL SETTINGS FOR TRAINING, includes hparam space\n",
    "experiment_name = 'cbh-hparam-tuning'\n",
    "CPU_COUNT = 10\n",
    "RAM_GB = 128\n",
    "hparams_for_mlflow['CPU Count'] = CPU_COUNT\n",
    "hparams_for_mlflow['Compute Memory'] = RAM_GB\n",
    "thread_count_for_dask = CPU_COUNT\n",
    "dataset_method = '1chunk'\n",
    "randomize_chunkwise_1chunk = False\n",
    "shuffle_train_data = False\n",
    "collate_fn = None # alt: cbh_data_definitions.dataloader_collate_with_dask\n",
    "num_workers_dataloader = 0 # alt: CPU_COUNT +-\n",
    "global_trail_number = 0\n",
    "max_time_for_trial = \"00:02:00:00\"  # dd:hh:mm:ss\n",
    "hparams_for_mlflow[\"Training timeout\"] = max_time_for_trial\n",
    "\n",
    "max_node_num_exclusive = 513\n",
    "factors_for_hparam_choice = [factor for factor in factors_of_chunk if (factor<3300 and factor>3)]\n",
    "mlp_search_space = {\n",
    "    \"epoch\": 1,\n",
    "    \"lr\": tune.quniform(0.0001, 0.01, 0.00005),\n",
    "    \"data_limit\": 4,#tune.randint(4, int(len(train_labels.chunks[0]) / 4)), # multiple chunk ind by chunklen: train_input.chunksize[0])\n",
    "    # \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"batch_size\": tune.choice(factors_for_hparam_choice),\n",
    "    \"arch_name\":\"MLP\",\n",
    "    \"hidden_layers\":tune.randint(1,11),\n",
    "    \"activation\":tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"input_size\":(train_input.shape[2] * train_input.shape[1]),\n",
    "    \"output_size\": train_input.shape[1],\n",
    "    \"layer_node_num\": tune.sample_from(lambda spec: 8*np.random.randint(1,int(max_node_num_exclusive/8), size=spec.config.hidden_layers)),\n",
    "    \"deterministic\":True,\n",
    "    \"chkpt_time\":datetime.timedelta(minutes=15),\n",
    "    \"max_time\":max_time_for_trial\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa35a93-c33d-4e81-b5f9-4a235028ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating experiment\n",
      "Caught\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "class MLFlowLogger(pl.loggers.MLFlowLogger): #overwrite mlflogger\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def after_save_checkpoint(self, model_checkpoint: pl.callbacks.ModelCheckpoint) -> None:\n",
    "        \"\"\"\n",
    "        Called after model checkpoint callback saves a new checkpoint.\n",
    "        \"\"\"\n",
    "        best_chkpt = torch.load(model_checkpoint.best_model_path)\n",
    "        checkpoint_for_mlflow = {\n",
    "            \"val loss\": float(best_chkpt['callbacks'][list(key for key in list(best_chkpt['callbacks'].keys()) if \"ModelCheckpoint\" in key)[0]]['current_score']),\n",
    "            \"train loss at step-1\": list(train_loss_metric.value for train_loss_metric in mlf_logger._mlflow_client.get_metric_history(run.info.run_id, \"Train loss\") if (int(train_loss_metric.step) == int(best_chkpt['global_step']-1)))[0],\n",
    "            \"global_step\": best_chkpt['global_step'],\n",
    "            \"model_state_dict\": best_chkpt['state_dict'],\n",
    "            \"checkpoint\": best_chkpt,\n",
    "        }\n",
    "        with TemporaryDirectory() as tmpdirname:\n",
    "            f_name = os.path.join(tmpdirname, f\"{run.info.run_id}-best_model_checkpoint-step_{best_chkpt['global_step']}.pt\")\n",
    "            torch.save(checkpoint_for_mlflow, f_name)\n",
    "            mlflow.log_artifact(f_name)\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_server_uri)\n",
    "# make vars global\n",
    "mlf_exp = None\n",
    "mlf_exp_id = None\n",
    "try: \n",
    "    print('Creating experiment')\n",
    "    mlf_exp_id = mlflow.create_experiment(experiment_name)\n",
    "    mlf_exp = mlflow.get_experiment(mlf_exp_id)\n",
    "except mlflow.exceptions.RestException as e:\n",
    "    print(\"Caught\")\n",
    "    if False:\n",
    "        print(e)\n",
    "    mlf_exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(\"Success\")\n",
    "\n",
    "mlp_search_space[\"mlflow\"] = {\n",
    "    \"tracking_uri\":mlflow_server_uri,\n",
    "    \"experiment_id\":mlf_exp_id,\n",
    "    \"experiment_name\":experiment_name,\n",
    "    # \"run_name\":(experiment_name+str(datetime.datetime.now())),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076ee9ae-57f6-4d17-90f3-e44b920af3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow_mixin\n",
    "def objective(ray_config):\n",
    "    # def model hparams with config\n",
    "    # print(ray_config)\n",
    "    # print(dir(ray_config))\n",
    "    # def data\n",
    "    print(\"Test print\")\n",
    "    datamodule = cbh_data_definitions.CBH_DataModule(\n",
    "        train_input, train_labels,\n",
    "        dev_input, dev_labels,\n",
    "        thread_count_for_dask,\n",
    "        ray_config['batch_size'],\n",
    "        num_workers = num_workers_dataloader,\n",
    "        collate_fn = collate_fn,\n",
    "        shuffle = shuffle_train_data,\n",
    "        randomize_chunkwise = randomize_chunkwise_1chunk,\n",
    "        method=dataset_method,\n",
    "    )\n",
    "    #def model\n",
    "    model = cbh_torch_MLP.CloudBaseMLP(\n",
    "        ray_config['input_size'],\n",
    "        ray_config['layer_node_num'],\n",
    "        ray_config['output_size'],\n",
    "        ray_config['hidden_layers'],\n",
    "        ray_config['activation'],\n",
    "        ray_config['lr'],\n",
    "    )\n",
    "    # def experiment naming\n",
    "    timestamp_template = '{dt.year:04d}{dt.month:02d}{dt.day:02d}T{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'\n",
    "    run_name_template = 'cbh_challenge_{network_name}_' + timestamp_template\n",
    "    global global_trail_number\n",
    "    current_run_name = run_name_template.format(network_name=model.__class__.__name__,\n",
    "                                                    dt=datetime.datetime.now()\n",
    "                                                   )\n",
    "    print(\"Finished model init\")\n",
    "    # begin mlflow experiment run\n",
    "    with mlflow.start_run(experiment_id=mlf_exp.experiment_id, run_name=current_run_name, nested=True) as run:\n",
    "        print(\"Started mlflow run\")\n",
    "        mlflow.pytorch.autolog()\n",
    "        mlf_logger = MLFlowLogger(experiment_name=experiment_name, tracking_uri=mlflow_server_uri, run_id=run.info.run_id)\n",
    "        print(\"Finished init logger\")\n",
    "        # define trainer\n",
    "        time_for_checkpoint = ray_config['chkpt_time']\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            train_time_interval=time_for_checkpoint,\n",
    "            dirpath=run.info.artifact_uri,\n",
    "            monitor=\"val_loss_mean\",\n",
    "            save_on_train_epoch_end=False,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks = [checkpoint_callback, RichProgressBar(), TuneReportCallback({'val_loss_mean': 'val_loss_mean'}, on=\"validation_end\")]\n",
    "        print(\"Finished define callbacks\")\n",
    "        trainer_hparams = {\n",
    "            'max_epochs':ray_config['epoch'],\n",
    "            'deterministic':ray_config['deterministic'],\n",
    "            'val_check_interval':10, # val every percentage of the epoch or an INT for after a number of batches\n",
    "            # 'devices':\"auto\",\n",
    "            # 'accelerator':\"auto\",\n",
    "            'max_time':ray_config['max_time'],\n",
    "            'replace_sampler_ddp':False,\n",
    "            'enable_checkpointing':True,\n",
    "            'strategy':None,\n",
    "            'callbacks':callbacks,\n",
    "            'logger':mlf_logger,\n",
    "        }\n",
    "        print(\"Finished init hparams kwargs\")\n",
    "\n",
    "        hparams_for_mlflow[\"Trainer hparams\"] = trainer_hparams\n",
    "        # hparams_for_mlflow[\"Trainer hparams\"]['callbacks'] = None # needs improvement, do not log function references, but parameters\n",
    "        mlf_logger.log_hyperparams(hparams_for_mlflow)\n",
    "        print(\"Finished log hparams mlflow\")\n",
    "        trainer = pl.Trainer(\n",
    "            **trainer_hparams\n",
    "        )\n",
    "        print(\"REACH all init before fit\")\n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "        path_to_save = '{dt.year:04d}{dt.month:02d}{dt.day:02d}-{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'.format(dt=datetime.datetime.now())\n",
    "        trainer.save_checkpoint(filepath=run.info.artifact_uri + f'/post_epoch_modelchkpt_{path_to_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ff4ee-30a6-4224-8833-da8301e9edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 17:01:07,017\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-11-14 17:01:13,799\tWARNING function_trainable.py:619 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-14 17:03:20 (running for 00:02:06.28)<br>Memory usage on this node: 31.7/251.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/48 CPUs, 0/0 GPUs, 0.0/156.81 GiB heap, 0.0/71.19 GiB objects<br>Result logdir: /home/h02/hsouth/ray_results/objective_2022-11-14_17-01-04<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc              </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_layers</th><th>layer_node_num   </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_f23af_00000</td><td>RUNNING </td><td>10.154.1.80:32965</td><td>relu        </td><td style=\"text-align: right;\">        1300</td><td style=\"text-align: right;\">              4</td><td>[440 480 208 456]</td><td style=\"text-align: right;\">0.005</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         110.479</td><td style=\"text-align: right;\">         3.1625</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Test print\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Finished model init\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Started mlflow run\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Finished init logger\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Finished define callbacks\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Finished init hparams kwargs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/loggers/mlflow.py:225: RuntimeWarning: Mlflow only allows parameters with up to 250 characters. Discard Trainer hparams/callbacks=[<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x2b0a101e7850>, <pytorch_lightning.callbacks.progress.rich_progress.RichProgressBar object at 0x2b0a102255a0>, <ray.tune.integration.pytorch_lightning.TuneReportCallback object at 0x2b0a10225600>]\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Finished log hparams mlflow\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m REACH all init before fit\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m │ 1 │ linears           │ ModuleList       │  531 K │\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Trainable params: 532 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Total params: 532 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m Total estimated model params size (MB): 2                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=32965)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-01-44\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 24.6245174407959\n",
      "  time_this_iter_s: 24.6245174407959\n",
      "  time_total_s: 24.6245174407959\n",
      "  timestamp: 1668445304\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.562089443206787\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n",
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-02-00\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 40.811378717422485\n",
      "  time_this_iter_s: 16.186861276626587\n",
      "  time_total_s: 40.811378717422485\n",
      "  timestamp: 1668445320\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.2160582542419434\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n",
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-02-17\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 57.543980836868286\n",
      "  time_this_iter_s: 16.7326021194458\n",
      "  time_total_s: 57.543980836868286\n",
      "  timestamp: 1668445337\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.1727938652038574\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n",
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-02-33\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 73.72983288764954\n",
      "  time_this_iter_s: 16.18585205078125\n",
      "  time_total_s: 73.72983288764954\n",
      "  timestamp: 1668445353\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.2032368183135986\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n",
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-02-53\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 93.58359837532043\n",
      "  time_this_iter_s: 19.8537654876709\n",
      "  time_total_s: 93.58359837532043\n",
      "  timestamp: 1668445373\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 5\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.182314395904541\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n",
      "Result for objective_f23af_00000:\n",
      "  date: 2022-11-14_17-03-10\n",
      "  done: false\n",
      "  experiment_id: a80334bd00584e7281528d4c609f96c6\n",
      "  hostname: expspicesrv066\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.154.1.80\n",
      "  pid: 32965\n",
      "  time_since_restore: 110.47902202606201\n",
      "  time_this_iter_s: 16.895423650741577\n",
      "  time_total_s: 110.47902202606201\n",
      "  timestamp: 1668445390\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 6\n",
      "  trial_id: f23af_00000\n",
      "  val_loss_mean: 3.162501573562622\n",
      "  warmup_time: 0.1783595085144043\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 17:03:21,116\tWARNING tune.py:686 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    }
   ],
   "source": [
    "searcher = OptunaSearch(metric=[\"val_loss_mean\"], mode=[\"min\"])\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=int(CPU_COUNT*(3/4)))\n",
    "num_hparam_trials = 1\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        # search_alg=algo,\n",
    "        num_samples=num_hparam_trials,\n",
    "    ),\n",
    "    param_space=mlp_search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49343e87-1030-400c-b6f3-2e01095ca66b",
   "metadata": {},
   "source": [
    "ensure mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfae81-73a0-4c54-8093-7ca61e4fa6a0",
   "metadata": {},
   "source": [
    "run study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082d416-89a5-4f64-a7a1-ae0b9efb8c48",
   "metadata": {},
   "source": [
    "eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-py-lightning Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-py-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
