{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a7bbf4-00f4-4b5d-97a9-d6d9138d0f8a",
   "metadata": {},
   "source": [
    "Make initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97e418c-c396-4b0d-9157-8f201bb9d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbh_data_definitions\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import mlflow\n",
    "from ray import tune\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.integration.mlflow import mlflow_mixin\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from pytorch_lightning.callbacks import (\n",
    "    RichProgressBar,\n",
    ")\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cbh_torch_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16443e15-8512-4a3f-a3eb-222764cd03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize some settings: mlflow, data directory, resources\n",
    "root_data_directory = pathlib.Path(os.environ[\"SCRATCH\"]) / \"cbh_data\"\n",
    "\n",
    "dev_data_path = root_data_directory / \"analysis_ready\" / \"dev_randomized.zarr\"\n",
    "training_data_path = root_data_directory / \"analysis_ready\" / \"train_randomized.zarr\"\n",
    "\n",
    "mlflow_command_line_run = \"\"\"\n",
    "    mlflow server --port 5001 --backend-store-uri sqlite:///mlflowSQLserver.db  --default-artifact-root ./mlflow_artifacts/\n",
    "\"\"\"\n",
    "mlflow_server_address = 'vld425'\n",
    "mlflow_server_port = 5001\n",
    "mlflow_server_uri = f'http://{mlflow_server_address}:{mlflow_server_port:d}'\n",
    "mlflow_artifact_root = pathlib.Path('./mlflow_artifacts/')\n",
    "\n",
    "hparams_for_mlflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14327c-6a24-4cb2-93da-f2697a78be1e",
   "metadata": {},
   "source": [
    "redefine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cba65e-0d9a-4924-93fb-47b4aacc9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n",
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 87.48 GiB </td>\n",
       "                        <td> 1.82 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (111820800, 70, 3) </td>\n",
       "                        <td> (2329600, 70, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Graph Layers </td>\n",
       "                        <td> 48 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"28\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"32\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"35\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"32\" y2=\"47\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"50\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"54\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"57\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"46\" y2=\"62\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"49\" y2=\"65\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"54\" y2=\"69\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"57\" y2=\"72\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"76\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"64\" y2=\"79\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"84\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"71\" y2=\"87\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"76\" y2=\"91\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"38\" y2=\"2\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"42\" y2=\"7\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"45\" y2=\"10\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"50\" y2=\"14\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"57\" y2=\"22\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"64\" y2=\"29\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"67\" y2=\"32\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"72\" y2=\"36\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"75\" y2=\"39\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"79\" y2=\"44\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"82\" y2=\"47\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"86\" y2=\"51\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"89\" y2=\"54\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"94\" y2=\"58\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"97\" y2=\"61\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"101\" y2=\"66\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">70</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">111820800</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(111820800, 70, 3), dtype=float32, chunksize=(2329600, 70, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init data\n",
    "(\n",
    "    train_input,\n",
    "    train_labels,\n",
    "    _,\n",
    ") = cbh_data_definitions.load_data_from_zarr(training_data_path)\n",
    "\n",
    "(\n",
    "    dev_input, \n",
    "    dev_labels, \n",
    "    _\n",
    ") = cbh_data_definitions.load_data_from_zarr(dev_data_path)\n",
    "\n",
    "# the cloud volume is not needed for the task, so isn't saved on the load\n",
    "# show a chunk\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edeea09a-8bd8-4313-a7ea-7fd8d37bf4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors of chunk:  [1, 2, 4, 5, 7, 8, 10, 13, 14, 16, 20, 25, 26, 28, 32, 35, 40, 50, 52, 56, 64, 65, 70, 80, 91, 100, 104, 112, 128, 130, 140, 160, 175, 182, 200, 208, 224, 256, 260, 280, 320, 325, 350, 364, 400, 416, 448, 455, 512, 520, 560, 640, 650, 700, 728, 800, 832, 896, 910, 1024, 1040, 1120, 1280, 1300, 1400, 1456, 1600, 1664, 1792, 1820, 2080, 2240, 2275, 2560, 2600, 2800, 2912, 3200, 3328, 3584, 3640, 4160, 4480, 4550, 5120, 5200, 5600, 5824, 6400, 6656, 7168, 7280, 8320, 8960, 9100, 10400, 11200, 11648, 12800, 13312, 14560, 16640, 17920, 18200, 20800, 22400, 23296, 25600, 29120, 33280, 35840, 36400, 41600, 44800, 46592, 58240, 66560, 72800, 83200, 89600, 93184, 116480, 145600, 166400, 179200, 232960, 291200, 332800, 465920, 582400, 1164800, 2329600]\n"
     ]
    }
   ],
   "source": [
    "# limit the data by a factor for less data in a tuning trial\n",
    "factors_of_chunk = [n for n in range(1, train_input.chunksize[0] + 1) if train_input.chunksize[0] % n == 0]\n",
    "print(\"Factors of chunk: \", factors_of_chunk)\n",
    "hparams_for_mlflow['Limited sample number'] =  -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbae1c6-5501-40e7-a2ba-4b1ef270f170",
   "metadata": {},
   "source": [
    "setup study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bd6235-7bfe-4bd1-aa06-ae1f5f86c223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'lr': <ray.tune.search.sample.Float object at 0x2b29ab1199c0>, 'data_limit': 4, 'batch_size': <ray.tune.search.sample.Categorical object at 0x2b29ab118490>, 'arch_name': 'MLP', 'hidden_layers': <ray.tune.search.sample.Integer object at 0x2b29ab118340>, 'activation': <ray.tune.search.sample.Categorical object at 0x2b29ab11ac80>, 'input_size': 210, 'output_size': 70, 'deterministic': False, 'chkpt_time': datetime.timedelta(seconds=900), 'max_time': '00:02:00:00', 'layer_node_number_0_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11aa70>, 'layer_node_number_1_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a9e0>, 'layer_node_number_2_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a950>, 'layer_node_number_3_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a440>, 'layer_node_number_4_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a3b0>, 'layer_node_number_5_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab118c40>, 'layer_node_number_6_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a590>, 'layer_node_number_7_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a620>, 'layer_node_number_8_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a6b0>, 'layer_node_number_9_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a740>, 'layer_node_number_10_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a7d0>, 'layer_node_number_11_div_8': <ray.tune.search.sample.Integer object at 0x2b29ab11a8f0>}\n"
     ]
    }
   ],
   "source": [
    "# DEFINE ALL SETTINGS FOR TRAINING, includes hparam space\n",
    "experiment_name = 'cbh-hparam-tuning'\n",
    "CPU_COUNT = 12\n",
    "RAM_GB = 128\n",
    "hparams_for_mlflow['CPU Count'] = CPU_COUNT\n",
    "hparams_for_mlflow['Compute Memory'] = RAM_GB\n",
    "thread_count_for_dask = CPU_COUNT\n",
    "dataset_method = '1chunk'\n",
    "randomize_chunkwise_1chunk = False\n",
    "shuffle_train_data = False\n",
    "collate_fn = None # alt: cbh_data_definitions.dataloader_collate_with_dask\n",
    "num_workers_dataloader = 0 # alt: CPU_COUNT +-\n",
    "global_trail_number = 0\n",
    "max_time_for_trial = \"00:02:00:00\"  # dd:hh:mm:ss\n",
    "hparams_for_mlflow[\"Training timeout\"] = max_time_for_trial\n",
    "\n",
    "max_node_num_exclusive = 513\n",
    "max_layers = 12\n",
    "factors_for_hparam_choice = [factor for factor in factors_of_chunk if (factor<3300 and factor>3)]\n",
    "mlp_search_space = {\n",
    "    \"epoch\": 1,\n",
    "    \"lr\": tune.quniform(0.0001, 0.01, 0.00005),\n",
    "    \"data_limit\": 4,\n",
    "    \"batch_size\": tune.choice(factors_for_hparam_choice),\n",
    "    \"arch_name\":\"MLP\",\n",
    "    \"hidden_layers\":tune.randint(1,max_layers),\n",
    "    \"activation\":tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"input_size\":(train_input.shape[2] * train_input.shape[1]),\n",
    "    \"output_size\": train_input.shape[1],\n",
    "    \"deterministic\":False,\n",
    "    \"chkpt_time\":datetime.timedelta(minutes=15),\n",
    "    \"max_time\":max_time_for_trial\n",
    "    \n",
    "}\n",
    "layer_pattern = 'layer_node_number_{layer_num}_div_8'\n",
    "for layer_num in range(max_layers):\n",
    "    mlp_search_space[layer_pattern.format(layer_num=layer_num)] = tune.randint(1,int(max_node_num_exclusive/8))\n",
    "print(mlp_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa35a93-c33d-4e81-b5f9-4a235028ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFlowLogger(pl.loggers.MLFlowLogger): #overwrite mlflogger\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def after_save_checkpoint(self, model_checkpoint: pl.callbacks.ModelCheckpoint) -> None:\n",
    "        \"\"\"\n",
    "        Called after model checkpoint callback saves a new checkpoint.\n",
    "        \"\"\"\n",
    "        best_chkpt = torch.load(model_checkpoint.best_model_path)\n",
    "        checkpoint_for_mlflow = {\n",
    "            \"val loss\": float(best_chkpt['callbacks'][list(key for key in list(best_chkpt['callbacks'].keys()) if \"ModelCheckpoint\" in key)[0]]['current_score']),\n",
    "            \"train loss at step-1\": list(train_loss_metric.value for train_loss_metric in mlf_logger._mlflow_client.get_metric_history(run.info.run_id, \"Train loss\") if (int(train_loss_metric.step) == int(best_chkpt['global_step']-1)))[0],\n",
    "            \"global_step\": best_chkpt['global_step'],\n",
    "            \"model_state_dict\": best_chkpt['state_dict'],\n",
    "            \"checkpoint\": best_chkpt,\n",
    "        }\n",
    "        with TemporaryDirectory() as tmpdirname:\n",
    "            f_name = os.path.join(tmpdirname, f\"{run.info.run_id}-best_model_checkpoint-step_{best_chkpt['global_step']}.pt\")\n",
    "            torch.save(checkpoint_for_mlflow, f_name)\n",
    "            mlflow.log_artifact(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076ee9ae-57f6-4d17-90f3-e44b920af3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_objective = False\n",
    "# @mlflow_mixin\n",
    "def objective(ray_config):\n",
    "    \n",
    "    mlflow.set_tracking_uri(mlflow_server_uri)\n",
    "    # make vars global\n",
    "    mlf_exp = None\n",
    "    mlf_exp_id = None\n",
    "    try: \n",
    "        if verbose_objective: print('Creating experiment')\n",
    "        mlf_exp_id = mlflow.create_experiment(experiment_name)\n",
    "        mlf_exp = mlflow.get_experiment(mlf_exp_id)\n",
    "    except mlflow.exceptions.RestException as e:\n",
    "        if verbose_objective: print(\"Caught\")\n",
    "        if False:\n",
    "            print(e)\n",
    "        mlf_exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if verbose_objective: print(\"Success\")\n",
    "    \n",
    "    datamodule = cbh_data_definitions.CBH_DataModule(\n",
    "        train_input, train_labels,\n",
    "        dev_input, dev_labels,\n",
    "        thread_count_for_dask,\n",
    "        ray_config['batch_size'],\n",
    "        num_workers = num_workers_dataloader,\n",
    "        collate_fn = collate_fn,\n",
    "        shuffle = shuffle_train_data,\n",
    "        randomize_chunkwise = randomize_chunkwise_1chunk,\n",
    "        method=dataset_method,\n",
    "    )\n",
    "    #def model\n",
    "    ff_nodes_strings = []\n",
    "    for key in ray_config:\n",
    "        if key.startswith(\"layer_node_number_\"):\n",
    "            ff_nodes_strings.append(key)\n",
    "    ff_nodes_strings = sorted(ff_nodes_strings)\n",
    "    ff_nodes = [(8*ray_config[ff_node_num]) for ff_node_num in ff_nodes_strings]\n",
    "    if verbose_objective: print(ray_config['hidden_layers'])\n",
    "    if verbose_objective: print(ff_nodes)\n",
    "    model = cbh_torch_MLP.CloudBaseMLP(\n",
    "        ray_config['input_size'],\n",
    "        ff_nodes,\n",
    "        ray_config['output_size'],\n",
    "        ray_config['hidden_layers'],\n",
    "        ray_config['activation'],\n",
    "        ray_config['lr'],\n",
    "    )                                       \n",
    "    if verbose_objective: print(\"Finished model init\")\n",
    "    timestamp_template = '{dt.year:04d}{dt.month:02d}{dt.day:02d}T{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'\n",
    "    run_name_template = 'cbh_challenge_{network_name}_' + timestamp_template\n",
    "    current_run_name = run_name_template.format(network_name=model.__class__.__name__,\n",
    "                                                    dt=datetime.datetime.now()\n",
    "                                                   )\n",
    "    # begin mlflow experiment run\n",
    "    with mlflow.start_run(experiment_id=mlf_exp.experiment_id, run_name=current_run_name) as run:\n",
    "        mlflow.pytorch.autolog()\n",
    "        mlf_logger = MLFlowLogger(experiment_name=experiment_name, tracking_uri=mlflow_server_uri, run_id=run.info.run_id)\n",
    "        if verbose_objective: print(\"Finished init logger\")\n",
    "        # define trainer\n",
    "        time_for_checkpoint = ray_config['chkpt_time']\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            train_time_interval=time_for_checkpoint,\n",
    "            dirpath=run.info.artifact_uri,\n",
    "            monitor=\"val_loss_mean\",\n",
    "            save_on_train_epoch_end=False,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks = [checkpoint_callback, RichProgressBar(), TuneReportCallback(\n",
    "            {\"val_loss_mean\": \"val_loss_mean\",},\n",
    "            on=\"validation_epoch_end\"\n",
    "            )\n",
    "        ]\n",
    "        if verbose_objective: print(\"Finished define callbacks\")\n",
    "        trainer_hparams = {\n",
    "            'max_epochs':ray_config['epoch'],\n",
    "            'deterministic':ray_config['deterministic'],\n",
    "            'val_check_interval':0.05, # val every percentage of the epoch or an INT for after a number of batches\n",
    "            'devices':\"auto\",\n",
    "            'accelerator':\"auto\",\n",
    "            'max_time':ray_config['max_time'],\n",
    "            'replace_sampler_ddp':False,\n",
    "            'enable_checkpointing':True,\n",
    "            'strategy':None,\n",
    "            'callbacks':callbacks,\n",
    "            'logger':mlf_logger,\n",
    "        }\n",
    "        if verbose_objective: print(\"Finished init hparams kwargs\")\n",
    "        hparams_for_mlflow['ray_config'] = ray_config\n",
    "        mlf_logger.log_hyperparams(hparams_for_mlflow)\n",
    "        if verbose_objective: print(\"Finished log hparams mlflow\")\n",
    "        if verbose_objective: print(trainer_hparams)\n",
    "        trainer = pl.Trainer(\n",
    "            **trainer_hparams\n",
    "        )\n",
    "        if verbose_objective: print(\"REACH all init before fit\")\n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "        path_to_save = '{dt.year:04d}{dt.month:02d}{dt.day:02d}-{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'.format(dt=datetime.datetime.now())\n",
    "        trainer.save_checkpoint(filepath=run.info.artifact_uri + f'/post_epoch_modelchkpt_{path_to_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475ff4ee-30a6-4224-8833-da8301e9edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 12:10:13,356\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "2022-11-18 12:10:22,260\tWARNING function_trainable.py:586 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/search/optuna/optuna_search.py:679: FutureWarning: DiscreteUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n",
      "  return ot.distributions.DiscreteUniformDistribution(\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/search/optuna/optuna_search.py:694: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n",
      "  return ot.distributions.IntUniformDistribution(\n",
      "\u001b[32m[I 2022-11-18 12:10:22,279]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-18 12:29:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:10.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>48.8/251.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 9.0/48 CPUs, 0/0 GPUs, 0.0/147.22 GiB heap, 0.0/67.08 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc              </th><th>activation  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_layers</th><th style=\"text-align: right;\">   layer_node_number_0_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_10\n",
       "_div_8</th><th style=\"text-align: right;\">   layer_node_number_11\n",
       "_div_8</th><th style=\"text-align: right;\">   layer_node_number_1_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_2_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_3_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_4_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_5_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_6_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_7_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_8_\n",
       "div_8</th><th style=\"text-align: right;\">   layer_node_number_9_\n",
       "div_8</th><th style=\"text-align: right;\">     lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_fa002b54</td><td>RUNNING </td><td>10.154.1.77:36425</td><td>tanh        </td><td style=\"text-align: right;\">         832</td><td style=\"text-align: right;\">              8</td><td style=\"text-align: right;\">35</td><td style=\"text-align: right;\">14</td><td style=\"text-align: right;\">57</td><td style=\"text-align: right;\">56</td><td style=\"text-align: right;\"> 6</td><td style=\"text-align: right;\">62</td><td style=\"text-align: right;\">52</td><td style=\"text-align: right;\">48</td><td style=\"text-align: right;\">56</td><td style=\"text-align: right;\"> 4</td><td style=\"text-align: right;\">47</td><td style=\"text-align: right;\">53</td><td style=\"text-align: right;\">0.0004 </td></tr>\n",
       "<tr><td>objective_fd8ede3c</td><td>RUNNING </td><td>10.154.1.77:37020</td><td>tanh        </td><td style=\"text-align: right;\">         130</td><td style=\"text-align: right;\">             10</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\"> 6</td><td style=\"text-align: right;\">53</td><td style=\"text-align: right;\">31</td><td style=\"text-align: right;\">11</td><td style=\"text-align: right;\">53</td><td style=\"text-align: right;\">62</td><td style=\"text-align: right;\">51</td><td style=\"text-align: right;\">17</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">51</td><td style=\"text-align: right;\">21</td><td style=\"text-align: right;\">0.0062 </td></tr>\n",
       "<tr><td>objective_0108445e</td><td>RUNNING </td><td>10.154.1.77:37276</td><td>tanh        </td><td style=\"text-align: right;\">           7</td><td style=\"text-align: right;\">              5</td><td style=\"text-align: right;\">63</td><td style=\"text-align: right;\">40</td><td style=\"text-align: right;\">59</td><td style=\"text-align: right;\">48</td><td style=\"text-align: right;\">61</td><td style=\"text-align: right;\">60</td><td style=\"text-align: right;\">17</td><td style=\"text-align: right;\">24</td><td style=\"text-align: right;\">40</td><td style=\"text-align: right;\">55</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\"> 5</td><td style=\"text-align: right;\">0.0084 </td></tr>\n",
       "<tr><td>objective_0461fd7a</td><td>RUNNING </td><td>10.154.1.77:37422</td><td>relu        </td><td style=\"text-align: right;\">        1600</td><td style=\"text-align: right;\">             11</td><td style=\"text-align: right;\">48</td><td style=\"text-align: right;\">55</td><td style=\"text-align: right;\">13</td><td style=\"text-align: right;\">22</td><td style=\"text-align: right;\"> 7</td><td style=\"text-align: right;\">45</td><td style=\"text-align: right;\">18</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">51</td><td style=\"text-align: right;\">22</td><td style=\"text-align: right;\">21</td><td style=\"text-align: right;\">52</td><td style=\"text-align: right;\">0.00905</td></tr>\n",
       "<tr><td>objective_07d5754a</td><td>RUNNING </td><td>10.154.1.77:37510</td><td>relu        </td><td style=\"text-align: right;\">        3200</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\">41</td><td style=\"text-align: right;\">56</td><td style=\"text-align: right;\">36</td><td style=\"text-align: right;\"> 7</td><td style=\"text-align: right;\">38</td><td style=\"text-align: right;\">12</td><td style=\"text-align: right;\">21</td><td style=\"text-align: right;\">49</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">39</td><td style=\"text-align: right;\">35</td><td style=\"text-align: right;\"> 9</td><td style=\"text-align: right;\">0.0099 </td></tr>\n",
       "<tr><td>objective_0b794186</td><td>RUNNING </td><td>10.154.1.77:37755</td><td>tanh        </td><td style=\"text-align: right;\">          13</td><td style=\"text-align: right;\">              4</td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">36</td><td style=\"text-align: right;\">48</td><td style=\"text-align: right;\">19</td><td style=\"text-align: right;\">31</td><td style=\"text-align: right;\">49</td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">52</td><td style=\"text-align: right;\">36</td><td style=\"text-align: right;\">43</td><td style=\"text-align: right;\">34</td><td style=\"text-align: right;\">63</td><td style=\"text-align: right;\">0.002  </td></tr>\n",
       "<tr><td>objective_0f41da1c</td><td>RUNNING </td><td>10.154.1.77:38620</td><td>tanh        </td><td style=\"text-align: right;\">         140</td><td style=\"text-align: right;\">              7</td><td style=\"text-align: right;\">20</td><td style=\"text-align: right;\">61</td><td style=\"text-align: right;\">59</td><td style=\"text-align: right;\">34</td><td style=\"text-align: right;\">25</td><td style=\"text-align: right;\">28</td><td style=\"text-align: right;\">49</td><td style=\"text-align: right;\">21</td><td style=\"text-align: right;\">28</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\"> 2</td><td style=\"text-align: right;\">37</td><td style=\"text-align: right;\">0.0002 </td></tr>\n",
       "<tr><td>objective_13fcf4a6</td><td>RUNNING </td><td>10.154.1.77:39023</td><td>tanh        </td><td style=\"text-align: right;\">         130</td><td style=\"text-align: right;\">              3</td><td style=\"text-align: right;\">18</td><td style=\"text-align: right;\">60</td><td style=\"text-align: right;\">27</td><td style=\"text-align: right;\"> 5</td><td style=\"text-align: right;\">12</td><td style=\"text-align: right;\">40</td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">37</td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">55</td><td style=\"text-align: right;\">13</td><td style=\"text-align: right;\">45</td><td style=\"text-align: right;\">0.00895</td></tr>\n",
       "<tr><td>objective_183b3c44</td><td>RUNNING </td><td>10.154.1.77:39109</td><td>tanh        </td><td style=\"text-align: right;\">         112</td><td style=\"text-align: right;\">              6</td><td style=\"text-align: right;\"> 5</td><td style=\"text-align: right;\">21</td><td style=\"text-align: right;\"> 1</td><td style=\"text-align: right;\">14</td><td style=\"text-align: right;\">60</td><td style=\"text-align: right;\">49</td><td style=\"text-align: right;\">14</td><td style=\"text-align: right;\">25</td><td style=\"text-align: right;\">35</td><td style=\"text-align: right;\">20</td><td style=\"text-align: right;\">36</td><td style=\"text-align: right;\"> 9</td><td style=\"text-align: right;\">0.00795</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: DiscreteUniformDistribution(high=0.01, low=0.0001, q=5e-05) is deprecated and internally converted to FloatDistribution(high=0.01, log=False, low=0.0001, step=5e-05). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: IntUniformDistribution(high=11, low=1, step=1) is deprecated and internally converted to IntDistribution(high=11, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: IntUniformDistribution(high=63, low=1, step=1) is deprecated and internally converted to IntDistribution(high=63, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m │ 1 │ linears           │ ModuleList       │  786 K │\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m Trainable params: 786 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m Total params: 786 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m Total estimated model params size (MB): 3                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m │ 1 │ linears           │ ModuleList       │  800 K │\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m Trainable params: 800 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m Total params: 800 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m Total estimated model params size (MB): 3                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m │ 1 │ linears           │ ModuleList       │  823 K │\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m Trainable params: 823 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m Total params: 823 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m Total estimated model params size (MB): 3                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m │ 1 │ linears           │ ModuleList       │  555 K │\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m Trainable params: 555 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m Total params: 555 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m Total estimated model params size (MB): 2                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m │ 1 │ linears           │ ModuleList       │  366 K │\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m Trainable params: 366 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m Total params: 366 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m Total estimated model params size (MB): 1                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m │ 1 │ linears           │ ModuleList       │  212 K │\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m Trainable params: 212 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m Total params: 212 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m Total estimated model params size (MB): 0                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m │ 1 │ linears           │ ModuleList       │  687 K │\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m Trainable params: 687 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m Total params: 687 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m Total estimated model params size (MB): 2                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m │ 1 │ linears           │ ModuleList       │  219 K │\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m Trainable params: 219 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m Total params: 219 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m Total estimated model params size (MB): 0                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m ┃   ┃ Name              ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m │ 0 │ layer_norm        │ LayerNorm        │    420 │\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m │ 1 │ linears           │ ModuleList       │  287 K │\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m │ 2 │ normalize_outputs │ Softmax          │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m │ 3 │ crossentropy_loss │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m └───┴───────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m Trainable params: 288 K                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m Total params: 288 K                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m Total estimated model params size (MB): 1                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=36425)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=37020)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=37276)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=37422)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=37510)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=37755)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=38620)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=39023)\u001b[0m   warning_cache.warn(m)\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='val_loss_mean')` could not find the monitored key in the returned metrics: ['Train loss', 'epoch', 'step']. HINT: Did you call `log('val_loss_mean', value)` in the `LightningModule`?\n",
      "\u001b[2m\u001b[36m(objective pid=39109)\u001b[0m   warning_cache.warn(m)\n",
      "2022-11-18 12:29:31,378\tWARNING tune.py:705 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2022-11-18 12:29:32,616\tERROR tune.py:773 -- Trials did not complete: [objective_fa002b54, objective_fd8ede3c, objective_0108445e, objective_0461fd7a, objective_07d5754a, objective_0b794186, objective_0f41da1c, objective_13fcf4a6, objective_183b3c44]\n",
      "2022-11-18 12:29:32,618\tINFO tune.py:777 -- Total run time: 1150.36 seconds (1150.04 seconds for the tuning loop).\n",
      "2022-11-18 12:29:32,628\tWARNING tune.py:783 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "*** SIGTERM received at time=1668774815 on cpu 19 ***\n",
      "PC: @     0x2b28ef59e0e3  (unknown)  epoll_wait\n",
      "    @     0x2b28ee981630  (unknown)  (unknown)\n",
      "[2022-11-18 12:33:35,814 E 29131 29131] logging.cc:361: *** SIGTERM received at time=1668774815 on cpu 19 ***\n",
      "[2022-11-18 12:33:35,814 E 29131 29131] logging.cc:361: PC: @     0x2b28ef59e0e3  (unknown)  epoll_wait\n",
      "[2022-11-18 12:33:35,814 E 29131 29131] logging.cc:361:     @     0x2b28ee981630  (unknown)  (unknown)\n"
     ]
    }
   ],
   "source": [
    "searcher = OptunaSearch(metric=[\"val_loss_mean\"], mode=[\"min\"])\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=int(CPU_COUNT*(3/4)))\n",
    "num_hparam_trials = 50\n",
    "# mlp_search_space[\"mlflow\"] = {\n",
    "#     \"tracking_uri\":mlflow_server_uri,\n",
    "#     \"experiment_id\":mlf_exp_id,\n",
    "#     \"experiment_name\":experiment_name,\n",
    "# }\n",
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=algo,\n",
    "        num_samples=num_hparam_trials,\n",
    "    ),\n",
    "    param_space=mlp_search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49343e87-1030-400c-b6f3-2e01095ca66b",
   "metadata": {},
   "source": [
    "ensure mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfae81-73a0-4c54-8093-7ca61e4fa6a0",
   "metadata": {},
   "source": [
    "run study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082d416-89a5-4f64-a7a1-ae0b9efb8c48",
   "metadata": {},
   "source": [
    "eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-py-lightning Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-py-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
