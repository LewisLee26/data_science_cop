{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a7bbf4-00f4-4b5d-97a9-d6d9138d0f8a",
   "metadata": {},
   "source": [
    "Make initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97e418c-c396-4b0d-9157-8f201bb9d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbh_data_definitions\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import mlflow\n",
    "from ray import tune\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.integration.mlflow import mlflow_mixin\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from pytorch_lightning.callbacks import (\n",
    "    RichProgressBar,\n",
    ")\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cbh_torch_MLP\n",
    "import cbh_torch_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16443e15-8512-4a3f-a3eb-222764cd03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize some settings: mlflow, data directory, resources\n",
    "root_data_directory = pathlib.Path(os.environ[\"SCRATCH\"]) / \"cbh_data\"\n",
    "\n",
    "dev_data_path = root_data_directory / \"analysis_ready\" / \"dev_randomized.zarr\"\n",
    "training_data_path = root_data_directory / \"analysis_ready\" / \"train_randomized.zarr\"\n",
    "\n",
    "mlflow_command_line_run = \"\"\"\n",
    "    mlflow server --port 5001 --backend-store-uri sqlite:///mlflowSQLserver.db  --default-artifact-root ./mlflow_artifacts/\n",
    "\"\"\"\n",
    "mlflow_server_address = 'vld425'\n",
    "mlflow_server_port = 5001\n",
    "mlflow_server_uri = f'http://{mlflow_server_address}:{mlflow_server_port:d}'\n",
    "mlflow_artifact_root = pathlib.Path('./mlflow_artifacts/')\n",
    "\n",
    "hparams_for_mlflow = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14327c-6a24-4cb2-93da-f2697a78be1e",
   "metadata": {},
   "source": [
    "redefine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cba65e-0d9a-4924-93fb-47b4aacc9bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n",
      "Loaded zarr, file information:\n",
      " Name              : /\n",
      "Type              : zarr.hierarchy.Group\n",
      "Read-only         : False\n",
      "Synchronizer type : zarr.sync.ThreadSynchronizer\n",
      "Store type        : zarr.storage.DirectoryStore\n",
      "No. members       : 2\n",
      "No. arrays        : 2\n",
      "No. groups        : 0\n",
      "Arrays            : cloud_base_label_y.zarr, humidity_temp_pressure_x.zarr\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 87.48 GiB </td>\n",
       "                        <td> 1.82 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (111820800, 70, 3) </td>\n",
       "                        <td> (2329600, 70, 3) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 2 Graph Layers </td>\n",
       "                        <td> 48 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float32 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"156\" height=\"146\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"25\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"28\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"32\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"35\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"40\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"43\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"32\" y2=\"47\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"50\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"39\" y2=\"54\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"57\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"46\" y2=\"62\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"49\" y2=\"65\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"54\" y2=\"69\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"57\" y2=\"72\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"76\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"64\" y2=\"79\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"68\" y2=\"84\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"71\" y2=\"87\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"76\" y2=\"91\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,96.00085180870013 10.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"38\" y2=\"2\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"42\" y2=\"7\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"45\" y2=\"10\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"50\" y2=\"14\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"53\" y2=\"17\" />\n",
       "  <line x1=\"32\" y1=\"22\" x2=\"57\" y2=\"22\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"39\" y1=\"29\" x2=\"64\" y2=\"29\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"67\" y2=\"32\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"72\" y2=\"36\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"75\" y2=\"39\" />\n",
       "  <line x1=\"54\" y1=\"44\" x2=\"79\" y2=\"44\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"82\" y2=\"47\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"86\" y2=\"51\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"89\" y2=\"54\" />\n",
       "  <line x1=\"68\" y1=\"58\" x2=\"94\" y2=\"58\" />\n",
       "  <line x1=\"71\" y1=\"61\" x2=\"97\" y2=\"61\" />\n",
       "  <line x1=\"76\" y1=\"66\" x2=\"101\" y2=\"66\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.41261651458248,0.0 106.00085180870013,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"106\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"96\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"70\" x2=\"106\" y2=\"96\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 106.00085180870013,70.58823529411765 106.00085180870013,96.00085180870013 80.58823529411765,96.00085180870013\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"93.294544\" y=\"116.000852\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"126.000852\" y=\"83.294544\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,126.000852,83.294544)\">70</text>\n",
       "  <text x=\"35.294118\" y=\"80.706734\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,80.706734)\">111820800</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(111820800, 70, 3), dtype=float32, chunksize=(2329600, 70, 3), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init data\n",
    "(\n",
    "    train_input,\n",
    "    train_labels,\n",
    "    _,\n",
    ") = cbh_data_definitions.load_data_from_zarr(training_data_path)\n",
    "\n",
    "(\n",
    "    dev_input, \n",
    "    dev_labels, \n",
    "    _\n",
    ") = cbh_data_definitions.load_data_from_zarr(dev_data_path)\n",
    "\n",
    "# the cloud volume is not needed for the task, so isn't saved on the load\n",
    "# show a chunk\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edeea09a-8bd8-4313-a7ea-7fd8d37bf4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors of chunk:  [1, 2, 4, 5, 7, 8, 10, 13, 14, 16, 20, 25, 26, 28, 32, 35, 40, 50, 52, 56, 64, 65, 70, 80, 91, 100, 104, 112, 128, 130, 140, 160, 175, 182, 200, 208, 224, 256, 260, 280, 320, 325, 350, 364, 400, 416, 448, 455, 512, 520, 560, 640, 650, 700, 728, 800, 832, 896, 910, 1024, 1040, 1120, 1280, 1300, 1400, 1456, 1600, 1664, 1792, 1820, 2080, 2240, 2275, 2560, 2600, 2800, 2912, 3200, 3328, 3584, 3640, 4160, 4480, 4550, 5120, 5200, 5600, 5824, 6400, 6656, 7168, 7280, 8320, 8960, 9100, 10400, 11200, 11648, 12800, 13312, 14560, 16640, 17920, 18200, 20800, 22400, 23296, 25600, 29120, 33280, 35840, 36400, 41600, 44800, 46592, 58240, 66560, 72800, 83200, 89600, 93184, 116480, 145600, 166400, 179200, 232960, 291200, 332800, 465920, 582400, 1164800, 2329600]\n"
     ]
    }
   ],
   "source": [
    "# limit the data by a factor for less data in a tuning trial\n",
    "factors_of_chunk = [n for n in range(1, train_input.chunksize[0] + 1) if train_input.chunksize[0] % n == 0]\n",
    "print(\"Factors of chunk: \", factors_of_chunk)\n",
    "hparams_for_mlflow['Limited sample number'] =  -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbae1c6-5501-40e7-a2ba-4b1ef270f170",
   "metadata": {},
   "source": [
    "setup study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bd6235-7bfe-4bd1-aa06-ae1f5f86c223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'lr': <ray.tune.search.sample.Float object at 0x2b1c665ac400>, 'data_limit': 4, 'batch_size': <ray.tune.search.sample.Categorical object at 0x2b1c665add50>, 'arch_name': 'MLP', 'hidden_layers': <ray.tune.search.sample.Integer object at 0x2b1c665adde0>, 'activation': <ray.tune.search.sample.Categorical object at 0x2b1c665ade70>, 'input_size': 210, 'output_size': 70, 'deterministic': False, 'chkpt_time': datetime.timedelta(seconds=900), 'max_time': '00:02:00:00', 'layer_node_number_0_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665ad5a0>, 'layer_node_number_1_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665ae290>, 'layer_node_number_2_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665ad930>, 'layer_node_number_3_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665ae560>, 'layer_node_number_4_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b9ab0>, 'layer_node_number_5_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b86d0>, 'layer_node_number_6_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b82b0>, 'layer_node_number_7_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665bb520>, 'layer_node_number_8_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665bb760>, 'layer_node_number_9_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b9d50>, 'layer_node_number_10_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b9cc0>, 'layer_node_number_11_div_8': <ray.tune.search.sample.Integer object at 0x2b1c665b9c30>}\n"
     ]
    }
   ],
   "source": [
    "# DEFINE ALL SETTINGS FOR TRAINING, includes hparam space\n",
    "experiment_name = 'cbh-hparam-tuning'\n",
    "CPU_COUNT = 12\n",
    "RAM_GB = 128\n",
    "hparams_for_mlflow['CPU Count'] = CPU_COUNT\n",
    "hparams_for_mlflow['Compute Memory'] = RAM_GB\n",
    "thread_count_for_dask = CPU_COUNT\n",
    "dataset_method = '1chunk'\n",
    "randomize_chunkwise_1chunk = False\n",
    "shuffle_train_data = False\n",
    "collate_fn = None # alt: cbh_data_definitions.dataloader_collate_with_dask\n",
    "num_workers_dataloader = 0 # alt: CPU_COUNT +-\n",
    "global_trail_number = 0\n",
    "max_time_for_trial = \"00:02:00:00\"  # dd:hh:mm:ss\n",
    "hparams_for_mlflow[\"Training timeout\"] = max_time_for_trial\n",
    "\n",
    "max_node_num_exclusive = 513\n",
    "max_layers = 12\n",
    "factors_for_hparam_choice = [factor for factor in factors_of_chunk if (factor<3300 and factor>3)]\n",
    "mlp_search_space = {\n",
    "    \"epoch\": 1,\n",
    "    \"lr\": tune.quniform(0.001, 0.01, 0.0005),\n",
    "    \"data_limit\": 4,\n",
    "    \"batch_size\": tune.choice(factors_for_hparam_choice),\n",
    "    \"arch_name\":\"MLP\",\n",
    "    \"hidden_layers\":tune.randint(1,max_layers),\n",
    "    \"activation\":tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"input_size\":(train_input.shape[2] * train_input.shape[1]),\n",
    "    \"output_size\": train_input.shape[1],\n",
    "    \"deterministic\":False,\n",
    "    \"chkpt_time\":datetime.timedelta(minutes=15),\n",
    "    \"max_time\":max_time_for_trial\n",
    "}\n",
    "lstm_search_space = {\n",
    "    \"epoch\": 1,\n",
    "    \"lr\": tune.quniform(0.0001, 0.005, 0.000005),\n",
    "    \"data_limit\": 4,\n",
    "    \"batch_size\": tune.choice(factors_for_hparam_choice),\n",
    "    \"arch_name\":\"LSTM\",\n",
    "    \"lstm_layers\":tune.randint(1,max_layers),\n",
    "    \"input_size\": train_input.shape[2],\n",
    "    \"output_size\": train_input.shape[1],\n",
    "    \"deterministic\":False,\n",
    "    \"chkpt_time\":datetime.timedelta(minutes=15),\n",
    "    \"max_time\":max_time_for_trial,\n",
    "    \"lstm_nodesize\":tune.randint(1,int(max_node_num_exclusive/8)),\n",
    "    \"height_dimension\": train_input.shape[1],\n",
    "    \"embed_size\": tune.randint(1,26),\n",
    "    \"BILSTM\": tune.choice([False, True]),\n",
    "    \"backward_lstm_differing_transitions\": tune.choice([False, True]),\n",
    "    \"batch_first\":True, \n",
    "    \"skip_connection\": tune.choice([False, True]),\n",
    "}\n",
    "layer_pattern = 'layer_node_number_{layer_num}_div_8'\n",
    "for layer_num in range(max_layers):\n",
    "    mlp_search_space[layer_pattern.format(layer_num=layer_num)] = tune.randint(1,int(max_node_num_exclusive/8))\n",
    "print(mlp_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa35a93-c33d-4e81-b5f9-4a235028ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFlowLogger(pl.loggers.MLFlowLogger): #overwrite mlflogger\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def after_save_checkpoint(self, model_checkpoint: pl.callbacks.ModelCheckpoint) -> None:\n",
    "        \"\"\"\n",
    "        Called after model checkpoint callback saves a new checkpoint.\n",
    "        \"\"\"\n",
    "        best_chkpt = torch.load(model_checkpoint.best_model_path)\n",
    "        checkpoint_for_mlflow = {\n",
    "            \"val loss\": float(best_chkpt['callbacks'][list(key for key in list(best_chkpt['callbacks'].keys()) if \"ModelCheckpoint\" in key)[0]]['current_score']),\n",
    "            \"train loss at step-1\": list(train_loss_metric.value for train_loss_metric in mlf_logger._mlflow_client.get_metric_history(run.info.run_id, \"Train loss\") if (int(train_loss_metric.step) == int(best_chkpt['global_step']-1)))[0],\n",
    "            \"global_step\": best_chkpt['global_step'],\n",
    "            \"model_state_dict\": best_chkpt['state_dict'],\n",
    "            \"checkpoint\": best_chkpt,\n",
    "        }\n",
    "        with TemporaryDirectory() as tmpdirname:\n",
    "            f_name = os.path.join(tmpdirname, f\"{run.info.run_id}-best_model_checkpoint-step_{best_chkpt['global_step']}.pt\")\n",
    "            torch.save(checkpoint_for_mlflow, f_name)\n",
    "            mlflow.log_artifact(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076ee9ae-57f6-4d17-90f3-e44b920af3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_objective = False\n",
    "# @mlflow_mixin\n",
    "def objective(ray_config):\n",
    "    \n",
    "    mlflow.set_tracking_uri(mlflow_server_uri)\n",
    "    # make vars global\n",
    "    mlf_exp = None\n",
    "    mlf_exp_id = None\n",
    "    try: \n",
    "        if verbose_objective: print('Creating experiment')\n",
    "        mlf_exp_id = mlflow.create_experiment(experiment_name)\n",
    "        mlf_exp = mlflow.get_experiment(mlf_exp_id)\n",
    "    except mlflow.exceptions.RestException as e:\n",
    "        if verbose_objective: print(\"Caught\")\n",
    "        if False:\n",
    "            print(e)\n",
    "        mlf_exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if verbose_objective: print(\"Success\")\n",
    "    \n",
    "    datamodule = cbh_data_definitions.CBH_DataModule(\n",
    "        train_input, train_labels,\n",
    "        dev_input, dev_labels,\n",
    "        thread_count_for_dask,\n",
    "        ray_config['batch_size'],\n",
    "        num_workers = num_workers_dataloader,\n",
    "        collate_fn = collate_fn,\n",
    "        shuffle = shuffle_train_data,\n",
    "        randomize_chunkwise = randomize_chunkwise_1chunk,\n",
    "        method=dataset_method,\n",
    "    )\n",
    "    #def model\n",
    "    if ray_config['arch_name'] == \"MLP\":\n",
    "        ff_nodes_strings = []\n",
    "        for key in ray_config:\n",
    "            if key.startswith(\"layer_node_number_\"):\n",
    "                ff_nodes_strings.append(key)\n",
    "        ff_nodes_strings = sorted(ff_nodes_strings)\n",
    "        ff_nodes = [(8*ray_config[ff_node_num]) for ff_node_num in ff_nodes_strings]\n",
    "        if verbose_objective: print(ray_config['hidden_layers'])\n",
    "        if verbose_objective: print(ff_nodes)\n",
    "        model = cbh_torch_MLP.CloudBaseMLP(\n",
    "            ray_config['input_size'],\n",
    "            ff_nodes,\n",
    "            ray_config['output_size'],\n",
    "            ray_config['hidden_layers'],\n",
    "            ray_config['activation'],\n",
    "            ray_config['lr'],\n",
    "        )\n",
    "    elif ray_config['arch_name'] == \"LSTM\":\n",
    "        lstm_node_dim = ray_config['lstm_nodesize'] * 8\n",
    "        model = cbh_torch_lstm.CloudBaseLSTM(\n",
    "            ray_config['input_size'], \n",
    "            ray_config['lstm_layers'], \n",
    "            lstm_node_dim, \n",
    "            ray_config['output_size'], \n",
    "            ray_config['height_dimension'],\n",
    "            ray_config['embed_size'], \n",
    "            ray_config['BILSTM'], \n",
    "            ray_config['backward_lstm_differing_transitions'], \n",
    "            ray_config['batch_first'], \n",
    "            ray_config['lr'], \n",
    "            ray_config['skip_connection'],\n",
    "        )\n",
    "    if verbose_objective: print(\"Finished model init\")\n",
    "    timestamp_template = '{dt.year:04d}{dt.month:02d}{dt.day:02d}T{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'\n",
    "    run_name_template = 'cbh_challenge_{network_name}_' + timestamp_template\n",
    "    current_run_name = run_name_template.format(network_name=model.__class__.__name__,\n",
    "                                                    dt=datetime.datetime.now()\n",
    "                                                   )\n",
    "    # begin mlflow experiment run\n",
    "    with mlflow.start_run(experiment_id=mlf_exp.experiment_id, run_name=current_run_name) as run:\n",
    "        mlflow.pytorch.autolog()\n",
    "        mlf_logger = MLFlowLogger(experiment_name=experiment_name, tracking_uri=mlflow_server_uri, run_id=run.info.run_id)\n",
    "        if verbose_objective: print(\"Finished init logger\")\n",
    "        # define trainer\n",
    "        time_for_checkpoint = ray_config['chkpt_time']\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            train_time_interval=time_for_checkpoint,\n",
    "            dirpath=run.info.artifact_uri,\n",
    "            monitor=\"val_loss_mean\",\n",
    "            save_on_train_epoch_end=False,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks = [checkpoint_callback, RichProgressBar(), TuneReportCallback(\n",
    "            {\"val_loss_mean\": \"val_loss_mean\",},\n",
    "            on=\"validation_end\"\n",
    "            )\n",
    "        ]\n",
    "        if verbose_objective: print(\"Finished define callbacks\")\n",
    "        trainer_hparams = {\n",
    "            'max_epochs':ray_config['epoch'],\n",
    "            'deterministic':ray_config['deterministic'],\n",
    "            'val_check_interval':25, # val every percentage of the epoch or an INT for after a number of batches\n",
    "            'devices':\"auto\",\n",
    "            'accelerator':\"auto\",\n",
    "            'max_time':ray_config['max_time'],\n",
    "            'replace_sampler_ddp':False,\n",
    "            'enable_checkpointing':True,\n",
    "            'strategy':None,\n",
    "            'callbacks':callbacks,\n",
    "            'logger':mlf_logger,\n",
    "        }\n",
    "        if verbose_objective: print(\"Finished init hparams kwargs\")\n",
    "        hparams_for_mlflow['ray_config'] = ray_config\n",
    "        mlf_logger.log_hyperparams(hparams_for_mlflow)\n",
    "        if verbose_objective: print(\"Finished log hparams mlflow\")\n",
    "        if verbose_objective: print(trainer_hparams)\n",
    "        trainer = pl.Trainer(\n",
    "            **trainer_hparams\n",
    "        )\n",
    "        if verbose_objective: print(\"REACH all init before fit\")\n",
    "        trainer.fit(model=model, datamodule=datamodule)\n",
    "        path_to_save = '{dt.year:04d}{dt.month:02d}{dt.day:02d}-{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'.format(dt=datetime.datetime.now())\n",
    "        trainer.save_checkpoint(filepath=run.info.artifact_uri + f'/post_epoch_modelchkpt_{path_to_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475ff4ee-30a6-4224-8833-da8301e9edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 15:52:11,674\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "2022-11-21 15:52:20,215\tWARNING function_trainable.py:586 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/search/optuna/optuna_search.py:679: FutureWarning: DiscreteUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n",
      "  return ot.distributions.DiscreteUniformDistribution(\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/search/optuna/optuna_search.py:694: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n",
      "  return ot.distributions.IntUniformDistribution(\n",
      "\u001b[32m[I 2022-11-21 15:52:20,228]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-21 15:52:48</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:28.19        </td></tr>\n",
       "<tr><td>Memory:      </td><td>44.9/251.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 4.0/48 CPUs, 0/0 GPUs, 0.0/104.53 GiB heap, 0.0/48.79 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_81b10e16</td><td style=\"text-align: right;\">           1</td><td>/home/h02/hsouth/ray_results/objective_2022-11-21_15-52-08/objective_81b10e16_3_BILSTM=False,arch_name=LSTM,backward_lstm_differing_transitions=False,batch_first=True,batch_size=325,chkpt_t_2022-11-21_15-52-30/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc              </th><th>BILSTM  </th><th>backward_lstm_differ\n",
       "ing_transitions      </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  embed_size</th><th style=\"text-align: right;\">      lr</th><th style=\"text-align: right;\">  lstm_layers</th><th style=\"text-align: right;\">  lstm_nodesize</th><th>skip_connection  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_7b589728</td><td>RUNNING </td><td>10.154.1.12:40005</td><td>False   </td><td>False</td><td style=\"text-align: right;\">         130</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">0.003085</td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             34</td><td>True             </td></tr>\n",
       "<tr><td>objective_7e9c038e</td><td>RUNNING </td><td>10.154.1.12:40072</td><td>True    </td><td>True </td><td style=\"text-align: right;\">         728</td><td style=\"text-align: right;\">          18</td><td style=\"text-align: right;\">0.002755</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">             30</td><td>False            </td></tr>\n",
       "<tr><td>objective_84f5cf76</td><td>RUNNING </td><td>10.154.1.12:40819</td><td>False   </td><td>False</td><td style=\"text-align: right;\">         455</td><td style=\"text-align: right;\">           9</td><td style=\"text-align: right;\">0.00428 </td><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">             46</td><td>False            </td></tr>\n",
       "<tr><td>objective_888df05a</td><td>RUNNING </td><td>10.154.1.12:41497</td><td>False   </td><td>True </td><td style=\"text-align: right;\">        1664</td><td style=\"text-align: right;\">          12</td><td style=\"text-align: right;\">0.000795</td><td style=\"text-align: right;\">            8</td><td style=\"text-align: right;\">             63</td><td>False            </td></tr>\n",
       "<tr><td>objective_8c222dda</td><td>PENDING </td><td>                 </td><td>True    </td><td>True </td><td style=\"text-align: right;\">        1792</td><td style=\"text-align: right;\">          14</td><td style=\"text-align: right;\">0.00132 </td><td style=\"text-align: right;\">            9</td><td style=\"text-align: right;\">             51</td><td>False            </td></tr>\n",
       "<tr><td>objective_81b10e16</td><td>ERROR   </td><td>10.154.1.12:40750</td><td>False   </td><td>False</td><td style=\"text-align: right;\">         325</td><td style=\"text-align: right;\">          17</td><td style=\"text-align: right;\">0.00476 </td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">             14</td><td>False            </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: DiscreteUniformDistribution(high=0.005, low=0.0001, q=5e-06) is deprecated and internally converted to FloatDistribution(high=0.005, log=False, low=0.0001, step=5e-06). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: IntUniformDistribution(high=11, low=1, step=1) is deprecated and internally converted to IntDistribution(high=11, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: IntUniformDistribution(high=63, low=1, step=1) is deprecated and internally converted to IntDistribution(high=63, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/optuna/distributions.py:766: FutureWarning: IntUniformDistribution(high=25, low=1, step=1) is deprecated and internally converted to IntDistribution(high=25, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m ┃   ┃ Name             ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m │ 0 │ LSTM_upward      │ LSTM             │  631 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m │ 1 │ height_embedding │ Embedding        │    630 │\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m │ 2 │ loss_fn_base     │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m │ 3 │ linearCap        │ Linear           │  401 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m │ 4 │ layer_norm       │ LayerNorm        │      6 │\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m └───┴──────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m Trainable params: 1.0 M                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m Total params: 1.0 M                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m Total estimated model params size (MB): 4                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /net/home/h02/hsouth/github_committing/data_science_cop/challenges/2021_cloud_base_height/cbh_torch_lstm.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   height = torch.tensor(torch.arange(0,70)).repeat((len(x),1)).reshape(len(x),70,1)\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m ┃   ┃ Name             ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 0 │ LSTM_upward      │ LSTM             │  1.1 M │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 1 │ LSTM_downward    │ LSTM             │  1.1 M │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 2 │ height_embedding │ Embedding        │  1.3 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 3 │ loss_fn_base     │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 4 │ linearCap        │ Linear           │  343 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m │ 5 │ layer_norm       │ LayerNorm        │      6 │\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m └───┴──────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m Trainable params: 2.5 M                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m Total params: 2.5 M                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m Total estimated model params size (MB): 10                                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m /net/home/h02/hsouth/github_committing/data_science_cop/challenges/2021_cloud_base_height/cbh_torch_lstm.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m   height = torch.tensor(torch.arange(0,70)).repeat((len(x),1)).reshape(len(x),70,1)\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:229: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:233: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:258: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:267: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m ┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m ┃   ┃ Name             ┃ Type             ┃ Params ┃\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m ┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m │ 0 │ LSTM_upward      │ LSTM             │  692 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m │ 1 │ height_embedding │ Embedding        │  1.2 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m │ 2 │ loss_fn_base     │ CrossEntropyLoss │      0 │\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m │ 3 │ linearCap        │ Linear           │  343 K │\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m │ 4 │ layer_norm       │ LayerNorm        │      6 │\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m └───┴──────────────────┴──────────────────┴────────┘\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m Trainable params: 1.0 M                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m Non-trainable params: 0                                                         \n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m Total params: 1.0 M                                                             \n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m Total estimated model params size (MB): 4                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m /net/home/h02/hsouth/github_committing/data_science_cop/challenges/2021_cloud_base_height/cbh_torch_lstm.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m   height = torch.tensor(torch.arange(0,70)).repeat((len(x),1)).reshape(len(x),70,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=40750)\u001b[0m \n",
      "\u001b[2m\u001b[36m(objective pid=40072)\u001b[0m \n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m Epoch 0/0                  7/82195944 0:00:14 • 1571   0.61it/s loss: 3.81      \n",
      "\u001b[2m\u001b[36m(objective pid=40005)\u001b[0m                                       days, 21:24:02            v_num: 0201     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 15:52:48,205\tERROR serialization.py:371 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "2022-11-21 15:52:48,212\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "2022-11-21 15:52:48,454\tERROR serialization.py:371 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "2022-11-21 15:52:48,457\tERROR trial_runner.py:993 -- Trial objective_81b10e16: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>date               </th><th>experiment_id                   </th><th>hostname      </th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_7b589728</td><td>2022-11-21_15-52-25</td><td>dc941316759c4b098b8d1bf43106f2d0</td><td>expspicesrv005</td><td>10.154.1.12</td><td style=\"text-align: right;\">40005</td><td style=\"text-align: right;\"> 1669045945</td><td>7b589728  </td></tr>\n",
       "<tr><td>objective_81b10e16</td><td>2022-11-21_15-52-36</td><td>4d17a51ebd7643fdbad701a65c75ed21</td><td>expspicesrv005</td><td>10.154.1.12</td><td style=\"text-align: right;\">40750</td><td style=\"text-align: right;\"> 1669045956</td><td>81b10e16  </td></tr>\n",
       "<tr><td>objective_84f5cf76</td><td>2022-11-21_15-52-42</td><td>554287fb032249258bee57faa42e079c</td><td>expspicesrv005</td><td>10.154.1.12</td><td style=\"text-align: right;\">40819</td><td style=\"text-align: right;\"> 1669045962</td><td>84f5cf76  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 15:52:48,495\tERROR serialization.py:371 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "2022-11-21 15:52:48,497\tERROR trial_runner.py:993 -- Trial objective_7b589728: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "\n",
      "2022-11-21 15:52:48,531\tERROR serialization.py:371 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "2022-11-21 15:52:48,533\tERROR trial_runner.py:993 -- Trial objective_84f5cf76: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/tblib/pickling_support.py\", line 26, in unpickle_exception\n",
      "    inst = func(*args)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/mlflow/exceptions.py\", line 96, in __init__\n",
      "    error_code = json.get(\"error_code\", ErrorCode.Name(INTERNAL_ERROR))\n",
      "AttributeError: 'str' object has no attribute 'get'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/_private/serialization.py\", line 275, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/h02/hsouth/ray_results/objective_2022-11-21_15-52-08\")`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:833\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m _ExecutorEventType\u001b[38;5;241m.\u001b[39mPG_READY:\n\u001b[0;32m--> 833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_pg_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m _ExecutorEventType\u001b[38;5;241m.\u001b[39mNO_RUNNING_TRIAL_TIMEOUT:\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:923\u001b[0m, in \u001b[0;36mTrialRunner._on_pg_ready\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    922\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to start trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_start_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m next_trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m Trial\u001b[38;5;241m.\u001b[39mERROR:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;66;03m# Only try to start another trial if previous trial startup\u001b[39;00m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# did not error (e.g. it just didn't start because its\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# placement group is not ready, yet).\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# Without this clause, this test fails:\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# test_trial_runner_pg.py::\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# TrialRunnerPlacementGroupHeterogeneousTest::\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# testResourceDeadlock\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     next_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial_executor\u001b[38;5;241m.\u001b[39mget_staged_trial()\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:915\u001b[0m, in \u001b[0;36mTrialRunner._on_pg_ready.<locals>._start_trial\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial_executor\u001b[38;5;241m.\u001b[39mstart_trial(trial):\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_start\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/callback.py:317\u001b[0m, in \u001b[0;36mCallbackList.on_trial_start\u001b[0;34m(self, **info)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks:\n\u001b[0;32m--> 317\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_start\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/logger.py:135\u001b[0m, in \u001b[0;36mLoggerCallback.on_trial_start\u001b[0;34m(self, iteration, trials, trial, **info)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_start\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m, iteration: \u001b[38;5;28mint\u001b[39m, trials: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m], trial: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo\n\u001b[1;32m    134\u001b[0m ):\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_trial_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/json.py:85\u001b[0m, in \u001b[0;36mJsonLoggerCallback.log_trial_start\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Update config\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Make sure logdir exists\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/json.py:120\u001b[0m, in \u001b[0;36mJsonLoggerCallback.update_config\u001b[0;34m(self, trial, config)\u001b[0m\n\u001b[1;32m    119\u001b[0m config_pkl \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(trial\u001b[38;5;241m.\u001b[39mlogdir, EXPR_PARAM_PICKLE_FILE)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_pkl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    121\u001b[0m     cloudpickle\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_configs[trial], f)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/tuner.py:243\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:295\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param_space)\n\u001b[0;32m--> 295\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:402\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    389\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    401\u001b[0m }\n\u001b[0;32m--> 402\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/tune.py:741\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 741\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:886\u001b[0m, in \u001b[0;36mTrialRunner.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot new trial to run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 886\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_and_handle_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_experiment_if_needed()\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:865\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TuneError(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 833, in _wait_and_handle_event\n    self._on_pg_ready(next_trial)\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 923, in _on_pg_ready\n    if not _start_trial(next_trial) and next_trial.status != Trial.ERROR:\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 915, in _start_trial\n    self._callbacks.on_trial_start(\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/callback.py\", line 317, in on_trial_start\n    callback.on_trial_start(**info)\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/logger.py\", line 135, in on_trial_start\n    self.log_trial_start(trial)\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/json.py\", line 85, in log_trial_start\n    self.update_config(trial, trial.config)\n  File \"/home/h02/hsouth/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/logger/json.py\", line 120, in update_config\n    with open(config_pkl, \"wb\") as f:\nOSError: [Errno 122] Disk quota exceeded\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m num_hparam_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      5\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m      6\u001b[0m     objective,\n\u001b[1;32m      7\u001b[0m     tune_config\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mTuneConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mlstm_search_space,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/py-lightning/lib/python3.10/site-packages/ray/tune/tuner.py:245\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\n\u001b[1;32m    246\u001b[0m             _TUNER_FAILED_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    247\u001b[0m                 path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner\u001b[38;5;241m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[1;32m    248\u001b[0m             )\n\u001b[1;32m    249\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     experiment_checkpoint_dir \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_tuner\u001b[38;5;241m.\u001b[39mget_experiment_checkpoint_dir\u001b[38;5;241m.\u001b[39mremote()\n\u001b[1;32m    253\u001b[0m     )\n",
      "\u001b[0;31mTuneError\u001b[0m: The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/h02/hsouth/ray_results/objective_2022-11-21_15-52-08\")`."
     ]
    }
   ],
   "source": [
    "searcher = OptunaSearch(metric=[\"val_loss_mean\"], mode=[\"min\"])\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=int(CPU_COUNT*(3/4)))\n",
    "num_hparam_trials = 50\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=algo,\n",
    "        num_samples=num_hparam_trials,\n",
    "    ),\n",
    "    param_space=lstm_search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49343e87-1030-400c-b6f3-2e01095ca66b",
   "metadata": {},
   "source": [
    "ensure mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfae81-73a0-4c54-8093-7ca61e4fa6a0",
   "metadata": {},
   "source": [
    "run study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082d416-89a5-4f64-a7a1-ae0b9efb8c48",
   "metadata": {},
   "source": [
    "eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-py-lightning Python (Conda)",
   "language": "python",
   "name": "conda-env-.conda-py-lightning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
