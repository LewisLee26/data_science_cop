{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ce0b86",
   "metadata": {},
   "source": [
    "# Design Pattern 2 - Embeddings (Chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9446b8a",
   "metadata": {},
   "source": [
    "## Introduction to Design Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf5656",
   "metadata": {},
   "source": [
    "In the previous pattern (Hashed Feature) we considered the case where one of our categorical input features has too many categories to sensibly handle with a one-hot encoding. In that case we used hashing which converts the categorical values to integers and groups them somewhat arbitrarily into fewer categories. It works reasonably well (especially for ordinal data), but there are better alternatives. \n",
    "\n",
    "Embeddings are a more sophisticated technique that also maps a set of inputs to fewer categories, but preserves the information relationship between them using a set of trainable weights. In this example we are going to use the built-in functionality of Tensorflow to show how to set up Embeddings with categorical data using a simple example from the original repo. Then follow up with a real-world example that handles text data.\n",
    "\n",
    "Note - this notebook is an introduction to Embeddings only and does not explain how to train them within a deep neural network -please look at the original example for more on this:\n",
    "\n",
    "* https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341dc76",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a6d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d716cd",
   "metadata": {},
   "source": [
    "## Simple example using sample data\n",
    "\n",
    "Let's look at the baby weight example data from the original repo notebook.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc169701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight_pounds  is_male  mother_age  plurality  gestation_weeks\n",
      "0       5.269048    false          15  Single(1)               28\n",
      "1       6.375769  Unknown          15  Single(1)               30\n",
      "2       7.749249     true          42  Single(1)               31\n",
      "3       1.250021     true          14   Twins(2)               25\n",
      "4       8.688418     true          15  Single(1)               31\n",
      "(999, 5)\n"
     ]
    }
   ],
   "source": [
    "baby_data = pd.read_csv(\"./data/babyweight_sample.csv\") \n",
    "print(baby_data.head(5))\n",
    "print(baby_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf51efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Single(1)' 'Twins(2)' 'Triplets(3)' 'Multiple(2+)' 'Quadruplets(4)']\n"
     ]
    }
   ],
   "source": [
    "print(baby_data.plurality.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55cd8f",
   "metadata": {},
   "source": [
    "You can see that the 'plurality' column is a categorical text variable, and we can assign numbers to the categories as it is ordinal data (there is a natural ordering from high to low) as shown in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487f6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {\n",
    "    'Single(1)': 0,\n",
    "    'Multiple(2+)': 1,\n",
    "    'Twins(2)': 2,\n",
    "    'Triplets(3)': 3,\n",
    "    'Quadruplets(4)': 4,\n",
    "    'Quintuplets(5)': 5\n",
    "}\n",
    "\n",
    "N_CLASSES = len(CLASSES)\n",
    "\n",
    "plurality_class = [CLASSES[plurality] for plurality in baby_data.plurality]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bc974",
   "metadata": {},
   "source": [
    "Let's print the first 5 examples...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a6af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Single(1)' 'Single(1)' 'Single(1)' 'Twins(2)' 'Single(1)']\n",
      "[0, 0, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "print(baby_data.plurality[:5].values)\n",
    "print(plurality_class[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489a410",
   "metadata": {},
   "source": [
    "Now we set up an embedding layer using Tensorflow!\n",
    "\n",
    "We supply arguments 'input_dim' and 'output_dim'. \n",
    "\n",
    "*  input_dim indicates the size of the vocabulary. For plurality this is 6.\n",
    "*  output_dim indicates the dimension of the embedding we want to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4051413",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 3\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=N_CLASSES,output_dim=EMBED_DIM, name='plurality_embedding')\n",
    "embeds = embedding_layer(tf.constant(plurality_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c968f",
   "metadata": {},
   "source": [
    "The variable 'embeds' is a two-dimensional tensor containing the embedding values for plurality for each row of data. Let's inspect it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63fa95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 3)\n",
      "tf.Tensor(\n",
      "[[-0.03500198 -0.00201459 -0.03848321]\n",
      " [-0.03500198 -0.00201459 -0.03848321]\n",
      " [-0.03500198 -0.00201459 -0.03848321]\n",
      " [ 0.03934843  0.04179189 -0.04929756]\n",
      " [-0.03500198 -0.00201459 -0.03848321]], shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeds.shape)\n",
    "print(embeds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501170c7",
   "metadata": {},
   "source": [
    "#### We can now use the embedding to learn the relationship between plurality and birth weight using a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f223c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_model = tf.keras.models.Sequential([\n",
    "        embedding_layer,\n",
    "        tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "baby_model.compile(\n",
    "   optimizer='adam',\n",
    "   loss='mse',\n",
    "   metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3dfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 13.8852 - mean_absolute_error: 3.0164\n",
      "Epoch 2/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.4370 - mean_absolute_error: 2.0043\n",
      "Epoch 3/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1972 - mean_absolute_error: 1.9706\n",
      "Epoch 4/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1760 - mean_absolute_error: 1.9693\n",
      "Epoch 5/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1778 - mean_absolute_error: 1.9696\n",
      "Epoch 6/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1616 - mean_absolute_error: 1.9654\n",
      "Epoch 7/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1768 - mean_absolute_error: 1.9654\n",
      "Epoch 8/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1677 - mean_absolute_error: 1.9666\n",
      "Epoch 9/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1697 - mean_absolute_error: 1.9652\n",
      "Epoch 10/10\n",
      "999/999 [==============================] - 2s 2ms/step - loss: 5.1697 - mean_absolute_error: 1.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae100da0d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_model.fit(tf.constant(plurality_class), baby_data.weight_pounds, batch_size=1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8676c",
   "metadata": {},
   "source": [
    "## Real world example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c0515",
   "metadata": {},
   "source": [
    "### Setting up an embedding for categorical land-use data\n",
    "\n",
    "Let's load the data and look at it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c52e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    land_cat_id                               land_cat_description\n",
      "0             1             discontinuous low density urban fabric\n",
      "1             2          discontinuous medium density urban fabric\n",
      "2             3           discontinuous dense density urban fabric\n",
      "3             4                            continuous urban fabric\n",
      "4             5  industrial commericial public military private...\n",
      "5             6        discontinuous very low density urban fabric\n",
      "6             7                                  green urban areas\n",
      "7             8                      sports and leisure facilities\n",
      "8             9                                           pastures\n",
      "9            10                           arable land annual crops\n",
      "10           11                                         port areas\n",
      "11           12             fast transit roads and associated land\n",
      "12           13                                isolated structures\n",
      "13           14                    other roads and associated land\n",
      "14           15                       railways and associated land\n",
      "15           16                  mineral extraction and dump sites\n",
      "16           17                           land without current use\n",
      "17           18                                            forests\n",
      "18           19                                           wetlands\n",
      "19           20                                              water\n",
      "20           21                                 construction sites\n",
      "21           22                                           airports\n",
      "22           23                   herbaceous vegetation moor grass\n",
      "23           24                    beaches dunes bare rock glacier\n",
      "(24, 2)\n"
     ]
    }
   ],
   "source": [
    "land_use_cats = pd.read_csv('./data/land_use_categories.csv')\n",
    "\n",
    "print(land_use_cats)\n",
    "\n",
    "print(land_use_cats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41e2c3",
   "metadata": {},
   "source": [
    "This is not ordinal data, so although we have a unique id ('land_cat_id') it is meaningless as an indicator of the relationship between categories.\n",
    "\n",
    "However, the text in 'land_cat_description' does contain information which we can use.\n",
    "\n",
    "We are going to use the text processing capabilities of Tensorflow to create an embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba1f1a",
   "metadata": {},
   "source": [
    "#### Convert the input text data to TF format\n",
    "\n",
    "Normally you would just use a representative sample from  a large dataset, but since our data is small we use all of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afc3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.constant(list(land_use_cats['land_cat_description']))\n",
    "labels = tf.constant(list(land_use_cats['land_cat_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dc88f",
   "metadata": {},
   "source": [
    "#### Instantiate a TextVectorization object and create the 'vocabulary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa31daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'urban', 'land', 'fabric', 'and', 'discontinuous', 'density', 'associated', 'sites', 'roads', 'low', 'areas', 'without', 'wetlands', 'water', 'very', 'vegetation', 'use', 'units', 'transit', 'structures', 'sports', 'rock', 'railways', 'public', 'private', 'port', 'pastures', 'other', 'moor', 'mineral', 'military', 'medium', 'leisure', 'isolated', 'industrial', 'herbaceous', 'green', 'grass', 'glacier', 'forests', 'fast', 'facilities', 'extraction', 'dunes', 'dump', 'dense', 'current', 'crops', 'continuous', 'construction', 'commericial', 'beaches', 'bare', 'arable', 'annual', 'airports'] 58\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab, len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96ced6",
   "metadata": {},
   "source": [
    "#### Create an Embedding model\n",
    "\n",
    "We can make the output size of the embedding anything we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35755214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, None, 3)           174       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 174\n",
      "Trainable params: 174\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 3\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=EMBED_DIM, name='embedding_layer')(x)\n",
    "outputs = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "land_use_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "print(land_use_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7929c",
   "metadata": {},
   "source": [
    "Now we can use the embedding to encode our data and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a92148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3)\n",
      "tf.Tensor(\n",
      "[[ 8.84700101e-03  9.66095924e-03  1.03270253e-02]\n",
      " [ 1.04699237e-02 -3.12716402e-05  1.48955649e-02]\n",
      " [ 1.11859245e-02  8.99216812e-03  2.35498250e-02]\n",
      " [-3.26201995e-03 -5.70814125e-03  1.04185445e-02]\n",
      " [-4.25992021e-03 -7.26009393e-03  3.13125644e-03]], shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoded_data = land_use_model(data)\n",
    "print(encoded_data.shape)\n",
    "print(encoded_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8726a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, None, 3)           174       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 178\n",
      "Trainable params: 178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 3\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=EMBED_DIM, name='embedding_layer')(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "land_use_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "print(land_use_model.summary())\n",
    "\n",
    "land_use_model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c26ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 203.8662 - mae: 12.4885\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 203.2480 - mae: 12.4621\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 202.5915 - mae: 12.4371\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 201.9628 - mae: 12.4105\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 201.2652 - mae: 12.3834\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 200.6031 - mae: 12.3534\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 199.8297 - mae: 12.3234\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 199.0731 - mae: 12.2907\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 198.2563 - mae: 12.2581\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 197.4237 - mae: 12.2221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fadc01df6d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "land_use_model.fit(data, labels, batch_size=1, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6fb40",
   "metadata": {},
   "source": [
    "### So how do we visualise a trained embedding?\n",
    "\n",
    "We can do that qualitatively using the  [tensorflow embedding projector](http://projector.tensorflow.org/)\n",
    "\n",
    "Firstly we need to extract out the trained embedding layer into a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cee4be",
   "metadata": {},
   "source": [
    "#### Baby data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac37852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "plurality_embedding_input (I [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "plurality_embedding (Embeddi (None, None, 3)           18        \n",
      "=================================================================\n",
      "Total params: 18\n",
      "Trainable params: 18\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "plurality_embedding = tf.keras.Model(inputs=baby_model.input,\n",
    "                outputs=baby_model.get_layer(\"plurality_embedding\").output)\n",
    "\n",
    "print(plurality_embedding.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b98f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.squeeze(plurality_embedding.predict(tf.constant(plurality_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858d670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('./babydata_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('./babydata_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(0,preds.shape[0]):\n",
    "    vec = preds[i].numpy()\n",
    "    out_m.write(str(baby_data.plurality[i]) + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2710ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-ml-python",
   "language": "python",
   "name": "test-ml-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
