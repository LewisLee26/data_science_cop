{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f238e8e8-e5e4-4bbb-b888-fc1864855125",
   "metadata": {},
   "source": [
    "# Design Pattern 14 - Distribution Strategy (Chapter 4 - Model Training Patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1a06a-bc7f-43e7-90ad-2c74ff75aa03",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "### Distribution strategy abstract definition\n",
    "Distribution strategy is the use of multiple devices either on the same machine, or across different machines to both overcome hardware limitations and speed up training for large models and data\n",
    "\n",
    "### Problem / Motivation\n",
    "1. \"it’s been shown that increasing the scale of deep learning, with respect to the number of training examples, the number of model parameters, or both leads to a significant increase in performance\"\n",
    "![](./Images/Compute_related_to_performance.PNG) |\n",
    "1. State of the art models can take enourmous amounts of time to train\n",
    "    1. Makes it hard to develop models, since results are unknown for a long time\n",
    "    1. Can increase the cost while using time based cloud compute resources, especially in the \"paying for a compute service\" model\n",
    "\n",
    "\n",
    "### Solution Overview \n",
    "\n",
    "As the title and definition hint: The solution is to split the effort of training the model across multiple machines\n",
    "\n",
    "Achieved under two different ways: Model Paralellism and Data Parallelism\n",
    "\n",
    "Model parallelism             |  Data parallelism\n",
    ":-------------------------:|:-------------------------:\n",
    "![](./Images/Model_parallelism.png)  |  ![](./Images/Data_parallelism.png)\n",
    "\n",
    "As the images may suggest, Model parallelism is splitting up the training by seperating model units across devices, and Data parallelism is splitting up the training by sending different data to *the same\\** model across devices. Devices must communicate to eachother to take and pass on their work.\n",
    "\n",
    "\\*To be explored\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "In any case for a distribution strategy, the main challenge is minimizing the time for device communication. Optimized file formats, prefetching communication data on the primary node to be served (or other ways to minimize resource idle time) can aid the issue.\n",
    "\n",
    "\n",
    "## Data Parallelism\n",
    "\n",
    "Data Parallelism can come in two forms: Synchronous - where at each step in model training, all nodes are updated to have the same model, and Asynchronous - All nodes communicate the the primary node in their own time to retrieve model weights and communicate results\n",
    "\n",
    "### Synchronous\n",
    "\n",
    "The workers train on different slices of input data in parallel and the gradient values are aggregated at the end of each training step. This is performed via an all-reduce algorithm\n",
    "\n",
    "![All Reduce Example](https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_allreduce_1.png)\n",
    "\n",
    "All-reduce will wait for all nodes to compute the gradient of the loss at each weight and average these gradients together for a single gradient at each weight with which to update the weight with. A copy of the new model is then sent back to all nodes for another step in training.\n",
    "\n",
    "The larger the model, the more gradients to communicate to the primary server, and the more weights to communicate back to the worker nodes. \n",
    "\n",
    "Some method of reducing the overhead of the I/O is to let the nodes communicate with eachother for faster propagation of results (Shown in the Data parallelism figure above). But overall, choosing a strategy that most suits available hardware can differ across infrastructure.\n",
    "\n",
    "Below, are the 3 strategies offered by tensorflow: `MultiWorkerMirrored` copies data across all nodes. `MirroredStrategy` Copies data on each GPU in a machine, and `CentralStorageStrategy` has the CPU communicate with each GPU without GPU-GPU information propagation\n",
    "\n",
    "![tf_ddp_strat_table](./Images/tensorflow_ddp_stratselection.PNG)\n",
    "\n",
    "The equivalent in PyTorch depends on the users own implementation of serving the data, ranging from strategies similar to CentralStorage to MultiWorkerMirroredStrategy with a CLI launch of PyTorch distributed specifying the master-node for model storage and all-reduce\n",
    "\n",
    "### Asynchronous\n",
    "\n",
    "![Parameter Server Arch](./Images/AsynParamServer.PNG)\n",
    "\n",
    "Models Weights and training slices are updated asyncronously typically with a Parameter-Server architecture: \"data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters\".\n",
    "\n",
    "The server node in this case does not perform the all-reduce operation, but will change the model after a certain interval / number of batches recieved back in order to update the weights of the model.\n",
    "\n",
    "Workers that fail and require a reboot will stop contributing to the training, but the training will still progress. This method loses determinism of results, and training slices may be entirely lost to a dead server. (Use case for virtual epochs!)\n",
    "\n",
    "The point of failure then has been reduced to the Server node alone, and worker nodes do not have to idle until remaining nodes finish work for the updated model like in Synchronous training.\n",
    "\n",
    "Keras implements `ParameterServerStrategy` for out of the box Async training. As before with PyTorch, user defined data serving with the torch helper functions can achieve similar results: [Tutorial](https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html)\n",
    "\n",
    "### ASync vs Sync\n",
    "\n",
    "Synchronous Data Parallelism             |  Asynchronous Data Parallelism\n",
    ":--------------------------------:|:--------------------------------:\n",
    "✔️ Increases Data Throughput | ✔️ Increases Data Throughput\n",
    "✔️ Faster Model Training Times | ✔️ Faster Model Training Times\n",
    "✔️ Deterministic  |  ❌ Non-Deterministic\n",
    "✔️ All work captured/utilized | ❌ Probability for work done on stale weights\n",
    "❌ Multiple Single Points of Failure | ✔️ High Failure Mitigation\n",
    "❌ Resource Idle Time | ✔️ Only I/O Speed Limited\n",
    "❌ I/O bottleneck | ❌ I/O bottleneck\n",
    "\n",
    "## Model Parallelism\n",
    "\n",
    "Partitioning parts of a network and their associated computations across multiple cores, the computation and memory workload is distributed across multiple devices.\n",
    "\n",
    "The I/O then happens between neurons of a network, hopefully the diagrams do well to explain this (Bold connctions must use I/O to communicate results to the necessary machines).\n",
    "![](./Images/Model_parallelism.png)\n",
    "\n",
    "\n",
    "## Extra Items of Note\n",
    "\n",
    "1. The solution, or which strategy to choose always depends on the hardware. Model Parallelism is more typical in model inference on small device scenarious, Async training on unreliable devices, Syncronous training for reproducible large model development etc. \n",
    "1. Custom hardware can assist in minimizing I/O waits: e.g. TPUs are optimized to communicate between eachother\n",
    "1. Batch size limit: a batch size too high misses convergence (see below)\n",
    "1. Sometimes performing both model and data parallelism is required/desired. [Mesh Tensorflow](https://arxiv.org/abs/1811.02084) attempts to provide a framework to tackle this issue.\n",
    "\n",
    "## Proof of concept results from the books\n",
    "\n",
    "![](./Images/more_distrib_workers_increases_throughput.PNG)\n",
    "![](./Images/more_workers_faster_convergence_distrib.PNG)\n",
    "![](./Images/Batch_size_too_large_causes_issues.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f07e1-7cb6-4958-bcd5-631d904e9620",
   "metadata": {},
   "source": [
    "## Code snippets from the book\n",
    "\n",
    "Probably due to the \"It depends\" nature of the avaiable hardware matching the distribution strategy, no concrete implementation of distributed strategy in practice for training is implemented, even on the github repository. Instead compiled are the code examples which point to ML framework's out of the box strategies for discussed strategies.\n",
    "\n",
    "Not mentioned but worth considering are: [Ray](https://docs.ray.io/en/latest/train/train.html), [DeepSpeed](https://www.deepspeed.ai/), or [Torch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_intermediate.html) . Other parallel ML frameworks are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb760-019b-4314-b287-2d5a195f3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirrored strategy tensorflow snippet\n",
    "\n",
    "# Simply replacing the strategy with another found in the docs \n",
    "# https://www.tensorflow.org/api_docs/python/tf/distribute \n",
    "# allows flexible training selection\n",
    "def tf_snippet():\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = tf.keras.Sequential([tf.keras.layers.Dense(32, input_shape=(5,)),\n",
    "                                     tf.keras.layers.Dense(16, activation='relu'),\n",
    "                                     tf.keras.layers.Dense(1)])\n",
    "        model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566c25e-83b5-4332-af32-aec77bb654fa",
   "metadata": {},
   "source": [
    "PyTorch script from shell example\n",
    "```\n",
    "python -m torch.distributed.launch --nproc_per_node=4 \\\n",
    "       --nnodes=16 --node_rank=3 --master_addr=\"192.168.0.1\" \\\n",
    "       --master_port=1234 my_pytorch.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f315e-3758-4c89-a866-17c314760202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch important components snippets\n",
    "\n",
    "# Docs: https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "\n",
    "def torch_snippets():\n",
    "    torch.distributed.init_process_group(backend=\"nccl\") # different backends per different infrastructure\n",
    "    local_rank = _ # local rank = rank of the distributed machine in machine hierarchy (read from script params)\n",
    "    device = torch.device(\"cuda:{}\".format(local_rank)) \n",
    "    model = _ # Some torch model\n",
    "    model = model.to(device)\n",
    "    ddp_model = DistributedDataParallel(model, device_ids=[local_rank],\n",
    "                                            output_device=local_rank)\n",
    "\n",
    "\n",
    "    sampler = DistributedSampler(dataset=trainds)\n",
    "    loader = DataLoader(dataset=trainds, batch_size=batch_size,\n",
    "                        sampler=sampler, num_workers=4)\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    for data in train_loader:\n",
    "        features, labels = data[0].to(device), data[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbd1d8-63c5-48de-b52f-7a5c5f535b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow TPU clusterresolver snippets\n",
    "\n",
    "# Docs: https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver\n",
    "def tf_TPU_snippets():\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "        tpu=tpu_address)\n",
    "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68703c-8ea3-4399-a1b1-e2725791900b",
   "metadata": {},
   "source": [
    "## Real world examples\n",
    "\n",
    "Data Distribution on SPICE: Failed, see ./dp14_demo/ \n",
    "Challenges: \n",
    "- The cluster must know the location of all devices (Sbatch jobs go to variable addresses)\n",
    "- Hard to debug networking issues on spice\n",
    "- lack of implementation examples from the book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad21e4-61c8-4926-9be1-56b186a758b4",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Model paralellism example paper: https://arxiv.org/pdf/1907.05019.pdf\n",
    "- Parameter Server Paper: http://web.eecs.umich.edu/~mosharaf/Readings/Parameter-Server.pdf\n",
    "- Online MLDP Book Chapter: https://learning.oreilly.com/library/view/machine-learning-design/9781098115777/ch04.html#why_it_works-id00313\n",
    "- ML Framework docs on model: https://docs.chainer.org/en/stable/chainermn/model_parallel/overview.html\n",
    "- Torch distributed insight paper: https://arxiv.org/pdf/2006.15704.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eba104-322e-43d1-bc77-b7b84ba3b60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-cop-torch Python (Conda)",
   "language": "python",
   "name": "conda-env-data-science-cop-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
